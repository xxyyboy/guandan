import os
import numpy as np
from pathlib import Path
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import json
from guandan_env import GuandanGame
from get_actions import enumerate_colorful_actions
import random
# åŠ è½½åŠ¨ä½œå…¨é›† M
with open("doudizhu_actions.json", "r", encoding="utf-8") as f:
    M = json.load(f)
action_dim = len(M)
RANKS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']
# æ„å»ºåŠ¨ä½œæ˜ å°„å­—å…¸
M_id_dict = {a['id']: a for a in M}
def find_entry_by_id(data, target_id):
    """è¿”å›åŒ¹é… id çš„æ•´ä¸ª JSON å¯¹è±¡"""
    for entry in data:
        if entry.get("id") == target_id:
            return entry
    return None

# Actor ç½‘ç»œå®šä¹‰
class ActorNet(nn.Module):
    def __init__(self, state_dim=3049, action_dim=action_dim, hidden_dim=512):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
    def forward(self, x, mask=None):
        logits = self.net(x)
        if mask is not None:
            logits = logits + (mask - 1) * 1e9
        return F.softmax(logits, dim=-1)

class CriticNet(nn.Module):
    def __init__(self, state_dim=3049, action_dim=1, hidden_dim=256):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
    def forward(self, x):
        value = self.net(x)
        return value


# åˆå§‹åŒ–æ¨¡å‹
actor = ActorNet()
critic= CriticNet()
actor_optimizer = optim.Adam(actor.parameters(), lr=1e-4)
critic_optimizer = optim.Adam(critic.parameters(), lr=1e-4)
gamma=0.9

# å°è¯•åŠ è½½å·²æœ‰æ¨¡å‹
def load_latest_models(actor, critic, model_dir="models"):
    model_files = sorted(Path(model_dir).glob("actor_ep*.pth"))
    if model_files:
        latest_actor_path = str(model_files[-1])
        ep = int(latest_actor_path.split("_ep")[1].split(".pth")[0])
        latest_critic_path = f"{model_dir}/critic_ep{ep}.pth"

        actor.load_state_dict(torch.load(latest_actor_path))
        print(f"âœ… åŠ è½½å·²æœ‰ actor æ¨¡å‹: {latest_actor_path}")

        if Path(latest_critic_path).exists():
            critic.load_state_dict(torch.load(latest_critic_path))
            print(f"âœ… åŠ è½½å·²æœ‰ critic æ¨¡å‹: {latest_critic_path}")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ° critic æ¨¡å‹: {latest_critic_path}")

        return ep
    return 0

# è°ƒç”¨åŠ è½½å‡½æ•°
initial_ep = load_latest_models(actor, critic)

# è®­ç»ƒå‡½æ•°
def train_on_batch(batch, gamma=0.99, device= None):
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    # å°†æ•°æ®ç»„ç»‡ä¸º tensor
    states = torch.tensor(np.array([s["state"] for s in batch]), dtype=torch.float32).to(device)
    actions = torch.tensor(np.array([s["action_id"] for s in batch]), dtype=torch.long).to(device)
    rewards = torch.tensor(np.array([s["reward"] for s in batch]), dtype=torch.float32).to(device)

    # è‡ªåŠ¨æ„é€  next_states å’Œ dones
    next_states = torch.zeros_like(states)
    next_states[:-1] = states[1:]  # t+1 çš„ state
    next_states[-1] = 0.0          # æœ€åä¸€æ¡æ— ä¸‹ä¸€çŠ¶æ€
    dones = torch.zeros(len(batch), dtype=torch.bool, device=device)
    dones[-1] = True               # æœ€åä¸€æ¡ä¸º done

    # === Critic ä¼°å€¼ ===
    values = critic(states).squeeze(-1)            # [batch]
    next_values = critic(next_states).squeeze(-1)  # [batch]
    next_values[dones] = 0.0

    # === Advantage ===
    advantages = rewards + gamma * next_values - values

    # === Critic Loss ===
    critic_loss = advantages.pow(2).mean()

    # === Actor Loss ===
    probs = actor(states)                          # [batch, action_dim]
    log_probs = torch.log(probs + 1e-8)
    chosen_log_probs = log_probs[range(len(batch)), actions]
    actor_loss = -(chosen_log_probs * advantages.detach()).mean()

    # === æ›´æ–°å‚æ•° ===
    actor_optimizer.zero_grad()
    actor_loss.backward()
    actor_optimizer.step()

    critic_optimizer.zero_grad()
    critic_loss.backward()
    critic_optimizer.step()

    return actor_loss.item(), critic_loss.item()

# æ¨¡æ‹Ÿè®­ç»ƒæµç¨‹
def run_training(episodes=1000):
    os.makedirs("models", exist_ok=True)
    for ep in range(initial_ep, initial_ep + episodes): # ä»ä¸Šæ¬¡çš„epç»§ç»­
        game = GuandanGame(verbose=False)
        memory = []
        game.log(f"\nğŸ® æ¸¸æˆå¼€å§‹ï¼å½“å‰çº§ç‰Œï¼š{RANKS[game.active_level - 2]}")

        while True:
            if game.is_game_over or len(game.history) > 200:  # å¦‚æœæ¸¸æˆç»“æŸï¼Œç«‹å³è·³å‡ºå¾ªç¯
                break
            player = game.players[game.current_player]
            active_players = 4 - len(game.ranking)

            # **å¦‚æœ Pass çš„äºº == "å½“å‰æœ‰æ‰‹ç‰Œçš„ç©å®¶æ•° - 1"ï¼Œå°±é‡ç½®è½®æ¬¡**
            if game.pass_count >= (active_players - 1) and game.current_player not in game.ranking:
                if game.jiefeng:
                    first_player = game.ranking[-1]
                    teammate = 2 if first_player == 0 else 0 if first_player == 2 else 3 if first_player == 1 else 1
                    game.log(f"\nğŸ†• è½®æ¬¡é‡ç½®ï¼ç©å®¶ {teammate + 1} æ¥é£ã€‚\n")
                    game.recent_actions[game.current_player] = []  # è®°å½•ç©ºåˆ—è¡¨
                    game.current_player = (game.current_player + 1) % 4
                    game.last_play = None  # âœ… å…è®¸æ–°çš„è‡ªç”±å‡ºç‰Œ
                    game.pass_count = 0  # âœ… Pass è®¡æ•°å½’é›¶
                    game.is_free_turn = True
                    game.jiefeng = False
                else:
                    game.log(f"\nğŸ†• è½®æ¬¡é‡ç½®ï¼ç©å®¶ {game.current_player + 1} å¯ä»¥è‡ªç”±å‡ºç‰Œã€‚\n")
                    game.last_play = None  # âœ… å…è®¸æ–°çš„è‡ªç”±å‡ºç‰Œ
                    game.pass_count = 0  # âœ… Pass è®¡æ•°å½’é›¶
                    game.is_free_turn = True

            if game.current_player == 0:
                # 1. æ¨¡å‹æ¨ç†
                state = game._get_obs()
                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
                mask = torch.tensor(game.get_valid_action_mask(player.hand, M, game.active_level,game.last_play)).unsqueeze(0)
                mask1 = mask.squeeze(0)
                probs = actor(state_tensor, mask)
                action_id = torch.multinomial(probs, 1).item()
                action_struct = M_id_dict[action_id]
                # 2. æšä¸¾æ‰€æœ‰åˆæ³•å‡ºç‰Œç»„åˆï¼ˆå¸¦èŠ±è‰²ï¼‰
                combos = enumerate_colorful_actions(action_struct, player.hand, game.active_level)
                if combos:
                    chosen_move = random.choice(combos)
                    if not chosen_move:
                        game.log(f"ç©å®¶ {game.current_player + 1} Pass")
                        game.pass_count += 1
                        game.recent_actions[game.current_player] = ['Pass']  # è®°å½• Pass
                    else:
                        # å¦‚æœ chosen_move ä¸ä¸ºç©ºï¼Œç»§ç»­è¿›è¡Œæ­£å¸¸çš„å‡ºç‰Œé€»è¾‘
                        game.last_play = chosen_move
                        game.last_player = game.current_player
                        for card in chosen_move:
                            player.played_cards.append(card)
                            player.hand.remove(card)
                        game.log(f"ç©å®¶ {game.current_player + 1} å‡ºç‰Œ: {' '.join(chosen_move)}")
                        game.recent_actions[game.current_player] = list(chosen_move)  # è®°å½•å‡ºç‰Œ
                        game.jiefeng = False
                        if not player.hand:  # ç©å®¶å‡ºå®Œç‰Œ
                            game.log(f"\nğŸ‰ ç©å®¶ {game.current_player + 1} å‡ºå®Œæ‰€æœ‰ç‰Œï¼\n")
                            game.ranking.append(game.current_player)
                            if len(game.ranking) <= 2:
                                game.jiefeng = True

                        game.pass_count = 0
                        if not player.hand:
                            game.pass_count -= 1

                        if game.is_free_turn:
                            game.is_free_turn = False
                else:
                    game.log(f"ç©å®¶ {game.current_player + 1} Pass")
                    game.pass_count += 1
                    game.recent_actions[game.current_player] = ['Pass']  # è®°å½• Pass


                # ä»idæ‰¾åˆ°JSONå‡ºç‰Œç»“æ„
                entry = find_entry_by_id(M,action_id)
                if action_id ==0 :# passæ— å¥–åŠ±
                    reward = 0
                else:# å¯¹é«˜çº§å‡ºç‰Œå¥–åŠ±ä¿®æ­£ï¼ˆ*2ï¼‰
                    reward = float(len(entry['points'])*(1/entry['logic_point']))
                    if 120 <= action_id <= 364 : reward += reward
                # æ‹†ç‚¸å¼¹ã€è¿å¯¹æƒ©ç½š
                mask2 = torch.tensor(game.get_valid_action_mask(player.hand, M, game.active_level, game.last_play)).squeeze(0)
                if 0 < action_id <= 48:# å‡ºå•ç‰Œã€å¯¹å­ã€ä¸‰å¼ 
                    for i in range(49,120):
                        if mask1[i] and not mask2[i]:# æ‹†äº†ç‚¸
                            reward -= 2
                            break
                    for i in range(330,375):
                        if mask1[i] and not mask2[i]:# æ‹†äº†é¡ºå­ã€è¿å¯¹
                            reward -= 2
                            break

                memory.append({"state": state, "action_id": action_id, "reward": reward})
                player.last_played_cards = game.recent_actions[game.current_player]
                game.current_player = (game.current_player + 1) % 4
            else:
                game.ai_play(player)  # å…¶ä»–äººç”¨éšæœº
            # **è®°å½•æœ€è¿‘ 5 è½®å†å²**
            if game.current_player == 0:
                round_history = [game.recent_actions[i] for i in range(4)]
                game.history.append(round_history)
                game.recent_actions = [['None'], ['None'], ['None'], ['None']]

        final_reward = game.upgrade_amount*(1 if game.winning_team == 1 else -1)
        if memory:  # ç¡®ä¿ memory ä¸ä¸ºç©º
            for i, s in enumerate(memory):
                s["reward"] += gamma ** (len(memory) - i - 1) * final_reward
            al,cl = train_on_batch(memory)
            if (ep + 1) % 20 == 0:
                print(f"Episode {ep + 1}, action_loss: {al:.4f},critic_loss: {cl:.4f}")


        if (ep + 1) % 200 == 0:
            torch.save(actor.state_dict(), f"models/actor_ep{ep + 1}.pth")
            torch.save(critic.state_dict(), f"models/critic_ep{ep + 1}.pth")

if __name__ == "__main__":
    run_training()
