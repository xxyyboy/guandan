"""
æ”¹è¿›çš„PPOç®—æ³•å®ç° - æ¼è›‹çº¸ç‰Œæ¸¸æˆAIè®­ç»ƒ
æ ¸å¿ƒç›®æ ‡ï¼šé™ä½AIé€‰æ‹©Passçš„æ¦‚ç‡ï¼Œé¼“åŠ±ç§¯æå‡ºç‰Œ
"""

import os
import sys
import math
import numpy as np
from pathlib import Path
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical
import json
from guandan_env import GuandanGame
from get_actions import enumerate_colorful_actions
import random
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime
import logging
from copy import deepcopy

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('training.log'),
        logging.StreamHandler()
    ]
)

# åŠ è½½åŠ¨ä½œé…ç½®
try:
    with open("guandan_actions.json", "r", encoding="utf-8") as f:
        M = json.load(f)
    action_dim = len(M)
    M_id_dict = {a['id']: a for a in M}
except FileNotFoundError:
    logging.error("æœªæ‰¾åˆ°guandan_actions.jsonæ–‡ä»¶")
    raise

RANKS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logging.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

class ReplayBuffer:
    """å­˜å‚¨è®­ç»ƒæ ·æœ¬çš„å¾ªç¯ç¼“å†²åŒº"""
    def __init__(self, capacity=10000):
        self.buffer = []  # å­˜å‚¨æ ·æœ¬çš„åˆ—è¡¨
        self.capacity = capacity # æœ€å¤§å®¹é‡
        self.position = 0
        
    def push(self, state, action, reward, next_state, done, log_prob):
        """æ·»åŠ æ–°æ ·æœ¬"""
        if len(self.buffer) < self.capacity:
            self.buffer.append(None)
        self.buffer[self.position] = (state, action, reward, next_state, done, log_prob)
        self.position = (self.position + 1) % self.capacity
        
    def sample(self, batch_size):
        """éšæœºé‡‡æ ·ä¸€æ‰¹æ ·æœ¬"""
        batch = random.sample(self.buffer, min(batch_size, len(self.buffer)))
        states, actions, rewards, next_states, dones, log_probs = map(np.stack, zip(*batch))
        return states, actions, rewards, next_states, dones, log_probs
        
    def __len__(self):
        return len(self.buffer)

class ResidualBlock(nn.Module):
    """æ®‹å·®å—ç»“æ„ï¼Œç¼“è§£æ·±å±‚ç½‘ç»œæ¢¯åº¦æ¶ˆå¤±"""
    def __init__(self, in_dim, out_dim, dropout_rate=0.1):
        super().__init__()
        self.fc1 = nn.Linear(in_dim, out_dim)
        self.bn1 = nn.BatchNorm1d(out_dim)
        self.ln1 = nn.LayerNorm(out_dim)
        self.fc2 = nn.Linear(out_dim, out_dim)
        self.bn2 = nn.BatchNorm1d(out_dim)
        self.ln2 = nn.LayerNorm(out_dim)
        self.dropout = nn.Dropout(dropout_rate)
        self.shortcut = nn.Sequential() # æ·å¾„è¿æ¥
        
        if in_dim != out_dim: # ç»´åº¦ä¸åŒ¹é…æ—¶ä½¿ç”¨æŠ•å½±
            self.shortcut = nn.Sequential(
                nn.Linear(in_dim, out_dim),
                nn.BatchNorm1d(out_dim),
                nn.LayerNorm(out_dim)
            )
            
    def forward(self, x):
        residual = self.shortcut(x)
        out = F.leaky_relu(self.ln1(self.bn1(self.fc1(x))), 0.1)
        out = self.dropout(out)
        out = self.ln2(self.bn2(self.fc2(out)))
        out += residual
        return F.leaky_relu(out, 0.1)

# ä¿®æ”¹SharedBackboneç±»
class SharedBackbone(nn.Module):
    def __init__(self, state_dim=3049, hidden_dim=1024):
        super().__init__()
        
        self.input_bn = nn.BatchNorm1d(state_dim)
        self.eval_mode = False
        
        # æ”¹è¿›çš„æ‰‹ç‰Œç¼–ç å™¨
        self.card_encoder = nn.Sequential(
            nn.Linear(108, 512),
            nn.LayerNorm(512),
            nn.GELU(),  # ä½¿ç”¨GELUæ›¿ä»£LeakyReLU
            nn.Dropout(0.2),
            nn.Linear(512, 512),
            nn.LayerNorm(512),
            nn.GELU()
        )
        
        # æ”¹è¿›çš„å†å²åŠ¨ä½œç¼–ç å™¨
        self.history_encoder = nn.LSTM(
            input_size=2160,
            hidden_size=512,
            num_layers=3,  # å¢åŠ å±‚æ•°
            batch_first=True,
            dropout=0.2,
            bidirectional=True
        )
        
        # æ”¹è¿›çš„åœºæ™¯ä¿¡æ¯ç¼–ç å™¨
        self.context_encoder = nn.Sequential(
            nn.Linear(781, 384),
            nn.LayerNorm(384),
            nn.GELU(),
            nn.Dropout(0.2),
            nn.Linear(384, 256),
            nn.LayerNorm(256),
            nn.GELU()
        )
        
        # å¤šå¤´æ³¨æ„åŠ›å±‚
        self.self_attention = nn.MultiheadAttention(
            embed_dim=512+1024+256,
            num_heads=8,
            dropout=0.1,
            batch_first=True
        )
        
        # æ”¹è¿›çš„èåˆå±‚
        self.fusion = nn.Sequential(
            nn.Linear(512+1024+256, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(0.2)
        )
        
        # æ›´æ·±çš„æ®‹å·®å—
        self.res_blocks = nn.ModuleList([
            ResidualBlock(hidden_dim, hidden_dim, dropout_rate=0.2)
            for _ in range(4)  # å¢åŠ æ®‹å·®å—æ•°é‡
        ])

    def forward(self, x):
        x = self.input_bn(x)
        
        # ç‰¹å¾æå–
        card_feat = self.card_encoder(x[..., :108])
        
        history_feat, _ = self.history_encoder(x[..., 108:2268].unsqueeze(1))
        history_feat = history_feat.squeeze(1)
        
        context_feat = self.context_encoder(x[..., 2268:])
        
        # ç‰¹å¾èåˆ
        combined_feat = torch.cat([card_feat, history_feat, context_feat], dim=-1)
        
        # è‡ªæ³¨æ„åŠ›å¤„ç†
        attn_out, _ = self.self_attention(
            combined_feat.unsqueeze(1),
            combined_feat.unsqueeze(1),
            combined_feat.unsqueeze(1)
        )
        combined_feat = combined_feat + attn_out.squeeze(1)  # æ®‹å·®è¿æ¥
        
        x = self.fusion(combined_feat)
        
        # æ®‹å·®å¤„ç†
        for res_block in self.res_blocks:
            x = res_block(x)
            
        return x
    
    def eval(self):
        """è®¾ç½®è¯„ä¼°æ¨¡å¼"""
        super().eval()
        self.eval_mode = True
        
    def train(self, mode=True):
        """è®¾ç½®è®­ç»ƒæ¨¡å¼"""
        super().train(mode)
        self.eval_mode = not mode
            
class ImprovedResNetActor(nn.Module):
    """æ”¹è¿›çš„ç­–ç•¥ç½‘ç»œ"""
    def __init__(self, backbone, action_dim=action_dim):
        super().__init__()
        self.backbone = backbone
        backbone_out_dim = backbone.res_blocks[-1].fc2.out_features
        
        # æ›´æ·±çš„ç­–ç•¥å¤´
        self.fc_policy = nn.Sequential(
            nn.Linear(backbone_out_dim, backbone_out_dim//2),
            nn.LayerNorm(backbone_out_dim//2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(backbone_out_dim//2, backbone_out_dim//4),
            nn.LayerNorm(backbone_out_dim//4),
            nn.GELU(),
            nn.Linear(backbone_out_dim//4, action_dim),
            nn.LayerNorm(action_dim)
        )
        
    def forward(self, x, mask=None):
        features = self.backbone(x)
        logits = self.fc_policy(features)
        
        # åŠ¨æ€PassæŠ‘åˆ¶ - æ ¹æ®è®­ç»ƒé˜¶æ®µè°ƒæ•´
        if self.training:
            pass_penalty = 1.2  # è®­ç»ƒæ—¶ä¸­ç­‰æŠ‘åˆ¶
        else:
            pass_penalty = 0.8  # è¯„ä¼°æ—¶è½»å¾®æŠ‘åˆ¶
            
        logits[..., 0] -= pass_penalty
        
        # åº”ç”¨åŠ¨ä½œæ©ç 
        if mask is not None:
            logits = logits + (mask.float() - 1) * 1e9
        
        probs = F.softmax(logits, dim=-1)
        return probs, logits

class ImprovedResNetCritic(nn.Module):
    """ä»·å€¼ç½‘ç»œ"""
    def __init__(self, backbone):
        super().__init__()
        self.backbone = backbone
        backbone_out_dim = backbone.res_blocks[-1].fc2.out_features
        self.fc_value = nn.Sequential(
            nn.Linear(backbone_out_dim, backbone_out_dim//2),
            nn.LayerNorm(backbone_out_dim//2),
            nn.ReLU(),
            nn.Dropout(0.1),  # Add dropout
            nn.Linear(backbone_out_dim//2, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Tanh() # è¾“å‡ºåœ¨[-1,1]èŒƒå›´
        )
        self._init_weights()
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.orthogonal_(m.weight, gain=1.0)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.0)
    def forward(self, x):
        features = self.backbone(x)
        value = self.fc_value(features)
        return value

def select_action(actor, state, mask, device, is_free_turn, ep):
    """æ”¹è¿›çš„åŠ¨ä½œé€‰æ‹©ç­–ç•¥4.0"""
    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)
    mask_tensor = torch.tensor(mask, dtype=torch.float32).unsqueeze(0).to(device)
    
    # æ›´å¹³ç¼“çš„æ¢ç´¢è¡°å‡æ›²çº¿
    explore_factor = max(0.15, 0.6 * (0.992 ** (ep // 50)))  # è¡°å‡æ›´æ…¢
    temp = max(0.7, 2.0 - ep/5000)  # æ›´é«˜çš„åˆå§‹æ¸©åº¦
    
    # åŠ¨æ€PassæŠ‘åˆ¶ç³»æ•°
    pass_suppress = 1.8 + (1 - ep/8000)  # éšè®­ç»ƒé€æ¸é™ä½
    
    with torch.no_grad():
        probs, logits = actor(state_tensor, mask_tensor)
        
        # å¼ºåŒ–PassæŠ‘åˆ¶
        if is_free_turn:
            logits[0, 0] = -float('inf')  # è‡ªç”±å‡ºç‰Œç¦æ­¢Pass
        else:
            # åŠ¨æ€Passæƒ©ç½š = åŸºç¡€å€¼ + è®­ç»ƒè¿›åº¦è°ƒæ•´
            pass_penalty = 1.5 + (1 - ep/6000)
            logits[0, 0] -= pass_penalty
            
        # ç‚¸å¼¹åŠ¨ä½œå¥–åŠ±
        bomb_mask = torch.zeros_like(logits)
        bomb_mask[:, 120:456] = 1  # ç‚¸å¼¹åŠ¨ä½œåŒºé—´
        logits += bomb_mask * (0.3 + ep/8000)  # éšè®­ç»ƒé€æ¸é‡è§†ç‚¸å¼¹
        
        # æ”¹è¿›çš„æ¢ç´¢æœºåˆ¶
        if random.random() < explore_factor:
            # åŸºäºQå€¼çš„æ¢ç´¢
            with torch.no_grad():
                q_values = critic(state_tensor).squeeze()
                valid_q = q_values * mask_tensor[0].float()
                valid_q[0] = -float('inf')  # æ’é™¤Pass
                
                # ä¼˜å…ˆæ¢ç´¢é«˜æ½œåŠ›åŠ¨ä½œ
                topk = min(5, (mask_tensor[0] > 0).sum())
                if topk > 0:
                    _, top_actions = valid_q.topk(topk)
                    action = top_actions[random.randint(0, topk-1)]
                    return action.item()

def validate_game_state(game):
    """éªŒè¯æ¸¸æˆçŠ¶æ€çš„åˆæ³•æ€§"""
    # æ£€æŸ¥å‡ºç‰Œæƒ
    if game.is_free_turn and game.last_play:
        logging.warning("çŠ¶æ€é”™è¯¯ï¼šè‡ªç”±å‡ºç‰Œå›åˆå­˜åœ¨last_play")
        game.last_play = []
        
    # æ£€æŸ¥æ’å
    if len(set(game.ranking)) != len(game.ranking):
        logging.error("çŠ¶æ€é”™è¯¯ï¼šæ’åé‡å¤")
        game.ranking = list(dict.fromkeys(game.ranking))
        
    # æ£€æŸ¥Passè®¡æ•°
    if game.pass_count > 4:
        logging.warning("çŠ¶æ€é”™è¯¯ï¼špass_countè¶…è¿‡4")
        game.pass_count = 0
        
    return True

def calculate_improved_reward(entry, player, mask, action_id, hand_size_before, game, ep):
    """æ”¹è¿›çš„å¥–åŠ±è®¡ç®—"""
    reward = 0.0
    progress = (27 - len(player.hand)) / 27  # æ¸¸æˆè¿›åº¦[0,1]
    
    # åŠ¨æ€åŸºç¡€å¥–åŠ± - éšæ¸¸æˆè¿›åº¦å¢åŠ 
    base_reward = 0.1 + progress * 0.2
    reward += base_reward
    
    # ç‚¸å¼¹ä½¿ç”¨å¥–åŠ±è°ƒæ•´
    bomb_bonus = {
        'bomb': 0.4 + progress*0.3,
        'straight_bomb': 0.7 + progress*0.4,
        'joker_bomb': 1.2  # é™ä½å¤©ç‹ç‚¸å¥–åŠ±
    }
    hand_size = len(player.hand)
    progress = (27 - hand_size) / 27
    
    # æ˜¥å¤©åˆ¤æ–­ - ç¬¬ä¸€è½®å°±å‡ºå®Œæ‰€æœ‰ç‰Œ
    if game.current_round == 1 and hand_size == 0:
        reward += 2.5  # æ˜¥å¤©é¢å¤–å¥–åŠ±
        game.log(f"ğŸ‰ æ˜¥å¤©ï¼ç©å®¶ {game.current_player + 1} åœ¨ç¬¬ä¸€è½®å°±å‡ºå®Œæ‰€æœ‰ç‰Œï¼")
    
    # Passå¤„ç†ä¼˜åŒ–
    if action_id == 0:
        base_penalty = 0.15 + (1 - progress) * 0.15  # è¿›ä¸€æ­¥é™ä½Passæƒ©ç½š
        phase_factor = max(0.2, 0.5 - ep/12000)
        penalty = -base_penalty * phase_factor
        
        # åœºæ™¯ç›¸å…³æƒ©ç½šè°ƒæ•´
        if game.is_free_turn:
            penalty *= 1.2
        elif game.last_play and len(game.last_play) >= 4:
            penalty *= 0.2  # å¤§å¹…é™ä½å¯¹å¤§ç‰Œçš„Passæƒ©ç½š
            
        reward += penalty
        
    else:
        # å‡ºç‰Œå¥–åŠ±ä¼˜åŒ–
        action_type = entry.get('type', '')
        cards_played = hand_size_before - hand_size
        
        # åŸºç¡€å¥–åŠ±(è€ƒè™‘å‰©ä½™ç‰Œæ•°)
        base_reward = 0.4 + progress * 0.3
        if hand_size <= len(player.hand) / 2:
            base_reward *= 1.2  # ç‰Œæ•°å°‘äºä¸€åŠæ—¶å¢åŠ å¥–åŠ±
            
        # ç‰Œå‹å¥–åŠ±ä¼˜åŒ–
        type_bonus = {
            'single': 0.2 + (1-progress)*0.2,
            'pair': 0.25 + progress*0.1,
            'trio': 0.3 + progress*0.15,
            'bomb': 0.4 + progress*0.2,  # æ™®é€šç‚¸å¼¹
            'straight_bomb': 0.6 + progress*0.3,  # é¡ºå­ç‚¸å¼¹
            'joker_bomb': 1.0,  # å¤©ç‹ç‚¸
            'sequence': 0.35 + len(game.last_play)*0.04,
            'spring': 5.0  # æ˜¥å¤©å¥–åŠ±
        }.get(action_type, 0.0)
        
        # æ§åˆ¶æƒå¥–åŠ±
        control_bonus = 0.0
        if game.last_player == 0:
            control_bonus = 0.2 + progress * 0.15
            
        # å…³é”®ç‰Œå¥–åŠ±
        key_card_bonus = 0.0
        if any(c[0] in ['A', '2'] for c in game.recent_actions[0]):
            key_card_bonus = 0.15 + progress * 0.1
            
        # é…åˆé˜Ÿå‹å¥–åŠ±
        teammate_bonus = 0.0
        teammate = (game.current_player + 2) % 4
        if game.last_player == teammate:
            teammate_bonus = 0.2
            
        reward += (base_reward + type_bonus + control_bonus + 
                  key_card_bonus + teammate_bonus)
        
        # ç»ˆå±€ä¼˜åŒ–
        if hand_size <= 3:
            reward += (4 - hand_size) * 0.4
            if hand_size == 0:  # å‡ºå®Œç‰Œé¢å¤–å¥–åŠ±
                reward += 2.0
                
    return reward


def compute_gae(rewards, values, next_values, dones, gamma=0.99, gae_lambda=0.95):
    batch_size = len(rewards)
    advantages = torch.zeros_like(rewards)
    gae = 0
    for t in reversed(range(batch_size)):
        if t == batch_size - 1:
            next_value = next_values[t]
        else:
            next_value = values[t + 1]
        delta = rewards[t] + gamma * next_value * (1 - dones[t].float()) - values[t]
        gae = delta + gamma * gae_lambda * (1 - dones[t].float()) * gae
        advantages[t] = gae
    returns = advantages + values
    # æ·»åŠ ä¼˜åŠ¿å€¼è£å‰ªå’Œæ›´ç¨³å®šçš„æ ‡å‡†åŒ–
    advantages = torch.clamp(advantages, -3.0, 3.0)
    adv_mean = advantages.mean()
    adv_std = advantages.std(unbiased=False) + 1e-7
    advantages = (advantages - adv_mean) / adv_std
    return advantages, returns

def save_checkpoint(backbone, actor, critic, optimizer, ep, model_dir="models"):
    os.makedirs(model_dir, exist_ok=True)
    checkpoint = {
        'backbone_state_dict': backbone.state_dict(),
        'actor_state_dict': actor.state_dict(),
        'critic_state_dict': critic.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'episode': ep
    }
    torch.save(checkpoint, f"{model_dir}/checkpoint_ep{ep}.pth")
    logging.info(f"ä¿å­˜æ£€æŸ¥ç‚¹: checkpoint_ep{ep}.pth")

def load_checkpoint(device, backbone, actor, critic, optimizer, model_dir="models"):
    model_files = sorted(Path(model_dir).glob("checkpoint_ep*.pth"))
    if not model_files:
        logging.info("æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
        return 0
    latest_checkpoint = str(model_files[-1])
    checkpoint = torch.load(latest_checkpoint, map_location=device)
    backbone.load_state_dict(checkpoint['backbone_state_dict'])
    actor.load_state_dict(checkpoint['actor_state_dict'])
    critic.load_state_dict(checkpoint['critic_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    ep = checkpoint['episode']
    logging.info(f"åŠ è½½æ£€æŸ¥ç‚¹: {latest_checkpoint}")
    return ep


def train_on_batch_ppo(states, actions, rewards, next_states, dones, old_log_probs,
                      backbone, actor, critic, target_critic, optimizer,
                      gamma=0.995, gae_lambda=0.95, device=device, ep=0):
    """ä¼˜åŒ–åçš„PPOè®­ç»ƒå‡½æ•°"""
    states = torch.FloatTensor(states).to(device)
    next_states = torch.FloatTensor(next_states).to(device)
    actions = torch.LongTensor(actions).to(device)
    rewards = torch.FloatTensor(rewards).to(device)
    dones = torch.BoolTensor(dones).to(device)
    old_log_probs = torch.FloatTensor(old_log_probs).to(device)

    # æ ‡å‡†åŒ–rewards
    rewards = torch.clamp(rewards, -5.0, 5.0)  # å…ˆè£å‰ªæç«¯å€¼
    rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)

    # è®¡ç®—ä¼˜åŠ¿å€¼å’Œå›æŠ¥
    with torch.no_grad():
        next_values = target_critic(next_states).squeeze(-1)
    values = critic(states).squeeze(-1)
    
    # ä½¿ç”¨TD(Î»)è®¡ç®—ä¼˜åŠ¿
    advantages = torch.zeros_like(rewards)
    returns = torch.zeros_like(rewards)
    gae = 0
    for t in reversed(range(len(rewards))):
        if t == len(rewards) - 1:
            next_value = next_values[t]
        else:
            next_value = values[t + 1]
        delta = rewards[t] + gamma * next_value * (1 - dones[t].float()) - values[t]
        gae = delta + gamma * gae_lambda * (1 - dones[t].float()) * gae
        advantages[t] = gae
        returns[t] = advantages[t] + values[t]
    
    # æ ‡å‡†åŒ–ä¼˜åŠ¿å€¼
    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

    # è®¡ç®—æ–°çš„åŠ¨ä½œæ¦‚ç‡
    probs, logits = actor(states)
    dist = Categorical(probs)
    new_log_probs = dist.log_prob(actions)
    entropy = dist.entropy().mean()
    
    # è®¡ç®—KLæ•£åº¦å¹¶åŠ¨æ€è°ƒæ•´clipèŒƒå›´
    kl_div = (old_log_probs - new_log_probs).mean()
    if kl_div > 0.02:
        clip_epsilon = max(0.1, 0.15)  # æ”¶ç´§clipèŒƒå›´
    else:
        clip_epsilon = max(0.1, 0.2 * (0.998 ** (ep // 50)))  # æ”¾å®½clipèŒƒå›´è¡°å‡
    
    # è®¡ç®—ç­–ç•¥æŸå¤±
    ratios = torch.exp(new_log_probs - old_log_probs)
    surr1 = ratios * advantages
    surr2 = torch.clamp(ratios, 1.0 - clip_epsilon, 1.0 + clip_epsilon) * advantages
    policy_loss = -torch.min(surr1, surr2).mean()
    
    # åŠ¨æ€è°ƒæ•´Passæƒ©ç½š
    pass_probs = probs[:, 0]
    pass_penalty_factor = max(0.02, 0.1 * (0.995 ** ep))  # é™ä½Passæƒ©ç½š
    pass_penalty = pass_penalty_factor * pass_probs.mean()
    policy_loss += pass_penalty
    
    # Value Lossè®¡ç®—ä¼˜åŒ–
    value_pred = critic(states)
    value_targets = returns.unsqueeze(-1)
    value_loss = F.smooth_l1_loss(value_pred, value_targets)
    value_loss = value_loss.clamp(-8.0, 8.0)  # æ‰©å¤§value lossèŒƒå›´
    
    # åŠ¨æ€ç†µç³»æ•°
    if ep < 200:  # å»¶é•¿åˆå§‹æ¢ç´¢æœŸ
        entropy_coef = 0.03  # æé«˜åˆå§‹ç†µç³»æ•°
    else:
        entropy_coef = max(0.005, 0.03 * (0.9995 ** ((ep-200)//50)))  # é™ä½ç†µè¡°å‡é€Ÿåº¦
    
    # åŠ¨æ€è°ƒæ•´value lossæƒé‡
    value_loss_weight = min(1.0, 0.5 + ep/1000)  # éšè®­ç»ƒè¿›ç¨‹å¢åŠ value lossæƒé‡
    
    # æ€»æŸå¤±
    total_loss = policy_loss + value_loss_weight * value_loss - entropy_coef * entropy
    
    # æ£€æŸ¥æŸå¤±å€¼
    if torch.isnan(total_loss) or torch.isinf(total_loss):
        logging.warning(f"Invalid loss detected: {total_loss}")
        return policy_loss.item(), value_loss.item(), entropy.item(), kl_div.item()
    
    # ä¼˜åŒ–å™¨æ­¥éª¤
    optimizer.zero_grad()
    total_loss.backward()
    
    # æ¢¯åº¦è£å‰ªå’Œç¼©æ”¾
    max_grad_norm = 1.0
    grad_norm = torch.nn.utils.clip_grad_norm_(backbone.parameters(), max_grad_norm)
    if grad_norm > max_grad_norm:
        for param in backbone.parameters():
            if param.grad is not None:
                param.grad.data.mul_(max_grad_norm / (grad_norm + 1e-6))
    
    torch.nn.utils.clip_grad_norm_(actor.parameters(), max_grad_norm)
    torch.nn.utils.clip_grad_norm_(critic.parameters(), max_grad_norm)
    
    optimizer.step()
    
    return policy_loss.item(), value_loss.item(), entropy.item(), kl_div.item()

def run_training(episodes=30000):
    """æ”¹è¿›çš„è®­ç»ƒæµç¨‹ - æ·»åŠ è¯¾ç¨‹å­¦ä¹ å’Œç›®æ ‡ç½‘ç»œ"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    adaptive_params = {
        'min_batch_size': 512,  # å¢å¤§åˆå§‹batch size
        'max_batch_size': 8192,  # å¢å¤§æœ€å¤§batch size
        'batch_growth_interval': 64,  # æ›´é¢‘ç¹è°ƒæ•´
        'current_batch_size': 512,
        'growth_step': 128  # å¢å¤§å¢é•¿æ­¥é•¿
    }
    
    # æ”¹è¿›çš„è¯¾ç¨‹å­¦ä¹ 
    def get_curriculum(ep, win_rate):
        if ep < 1000:  # å»¶é•¿åˆå§‹è®­ç»ƒé˜¶æ®µ
            return {'level': (2,4), 'opponent': 'random'}
        elif ep < 3000:
            if win_rate < 0.45:
                return {'level': (3,6), 'opponent': 'random'}
            else:
                return {'level': (4,7), 'opponent': 'rule_based'}
        elif ep < 6000:
            if win_rate < 0.55:
                return {'level': (5,8), 'opponent': 'rule_based'}
            else:
                return {'level': (6,10), 'opponent': 'self'}
        else:
            return {'level': (8,14), 'opponent': 'self'}
    
    # å¯ç”¨cuDNN benchmarkæ¨¡å¼
    torch.backends.cudnn.benchmark = True
    
    # åˆå§‹åŒ–ç½‘ç»œ
    backbone = SharedBackbone().to(device)
    actor = ImprovedResNetActor(backbone).to(device)
    critic = ImprovedResNetCritic(backbone).to(device)
    
    # åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œï¼ˆç”¨äºç¨³å®šè®­ç»ƒï¼‰
    target_backbone = SharedBackbone().to(device)
    target_critic = ImprovedResNetCritic(target_backbone).to(device)
    
    # åŒæ­¥åˆå§‹å‚æ•°
    target_backbone.load_state_dict(backbone.state_dict())
    target_critic.load_state_dict(critic.state_dict())
    
    # ä¼˜åŒ–å™¨åˆ†ç»„è®¾ç½®
    optimizer_params = []
    
    # ä¼˜åŒ–å™¨è®¾ç½®
    optimizer_params = [
        {'params': [p for n,p in actor.named_parameters() 
                if not n.startswith('backbone.')],
         'lr': 3e-5,
         'weight_decay': 1e-5},
        {'params': [p for n,p in critic.named_parameters()
                if not n.startswith('backbone.')],
        'lr': 1.5e-4},  # é™ä½criticå­¦ä¹ ç‡
        {'params': backbone.parameters(),
        'lr': 5e-5}   # é™ä½backboneå­¦ä¹ ç‡
    ]

    optimizer = optim.AdamW(optimizer_params, weight_decay=1e-5)

    # å­¦ä¹ ç‡è°ƒåº¦å™¨ä¼˜åŒ–
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, 
        mode='min',
        patience=150,    # å¢åŠ patience
        factor=0.95,     # é™ä½å­¦ä¹ ç‡è¡°å‡ç³»æ•°
        min_lr=5e-6,
        verbose=True
    )
    
    memory = ReplayBuffer(capacity=10000)
    writer = SummaryWriter(f'runs/guandan_ppo_{datetime.now().strftime("%Y%m%d_%H%M%S")}')
    initial_ep = load_checkpoint(device, backbone, actor, critic, optimizer)
    best_reward = float('-inf')
    
    # åˆå§‹åŒ–è®­ç»ƒæŒ‡æ ‡
    policy_loss = float('inf')  # åˆå§‹åŒ–ä¸ºä¸€ä¸ªå¤§å€¼
    value_loss = 0
    entropy = 0
    kl_div = 0
    
    def soft_update(target, source, tau=0.005):
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(
                target_param.data * (1.0 - tau) + param.data * tau
            )
    try:
        for ep in range(initial_ep, initial_ep + episodes):
            game = GuandanGame(verbose=True)
            episode_reward = 0
            episode_steps = 0
            pass_penalty_given = False  # é˜²æ­¢å¤šæ¬¡æƒ©ç½š
            while not game.is_game_over and len(game.history) <= 200:
                # å¼ºåŒ–è¿ç»­Passæ£€æµ‹å’Œå¤„ç†
                if game.pass_count >= 2:  # é™ä½æ£€æµ‹é˜ˆå€¼
                    # è®°å½•è¿ç»­Passè½®æ¬¡
                    consecutive_pass_rounds = game.pass_count
                    
                    # é‡ç½®å‡ºç‰Œæƒç»™æœ€åå‡ºç‰Œçš„ç©å®¶
                    if game.last_player is not None:
                        game.current_player = game.last_player
                        game.log(f"ç©å®¶ {game.last_player + 1} è·å¾—æ–°ä¸€è½®å‡ºç‰Œæƒï¼ˆè¿ç»­{consecutive_pass_rounds}æ¬¡Passåï¼‰")
                    
                    # åº”ç”¨å…¨å±€æƒ©ç½šï¼ˆæ¯è½®Passéƒ½æƒ©ç½šï¼‰
                    global_penalty = -0.5 * consecutive_pass_rounds
                    if len(memory) > 0:
                        memory.push(memory.buffer[-1][0], 0, global_penalty, 
                                  memory.buffer[-1][0], False, 0.0)
                        episode_reward += global_penalty
                    
                    # é‡ç½®çŠ¶æ€
                    game.last_play = []
                    game.pass_count = 0
                    game.recent_actions = [['None'] for _ in range(4)]
                    continue
    
                # æ£€æŸ¥è¿ç»­4æ¬¡Passæƒ©ç½š
                if game.pass_count >= 4 and not pass_penalty_given:
                    # å¯¹æ‰€æœ‰ç©å®¶ï¼ˆè¿™é‡Œåªè®­ç»ƒä¸»æ™ºèƒ½ä½“ï¼Œrewardè®°å½•åœ¨memoryï¼‰
                    penalty = -2.0
                    # ä»…å¯¹ä¸»æ™ºèƒ½ä½“åšè®°å½•
                    if len(memory) > 0:
                        memory.push(memory.buffer[-1][0], 0, penalty, memory.buffer[-1][0], False, 0.0)
                        episode_reward += penalty
                    # æ—¥å¿—æç¤º
                    logging.info("æ‰€æœ‰ç©å®¶è¿ç»­4æ¬¡Passï¼Œç»™äºˆæ‰€æœ‰ç©å®¶æƒ©ç½šï¼")
                    # æ¸…ç©ºpass_countå’Œrecent_actionsï¼Œè¿›å…¥æ–°ä¸€è½®
                    game.pass_count = 0
                    game.recent_actions = [['None'] for _ in range(4)]
                    pass_penalty_given = True
                else:
                    pass_penalty_given = False

                # ç©å®¶0ï¼šè®­ç»ƒä¸­çš„PPOæ™ºèƒ½ä½“ï¼ˆä¸»è®­ç»ƒå¯¹è±¡ï¼‰
                # ç©å®¶1-3ï¼šæ¸¸æˆå†…ç½®çš„è§„åˆ™å‹AIå¯¹æ‰‹
                # é€šè¿‡current_playerè½®è½¬æœºåˆ¶å®ç°å›åˆåˆ¶å‡ºç‰Œ
                if game.current_player == 0:
                    state = game._get_obs() #ç©å®¶çŠ¶æ€
                    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)
                    mask = torch.tensor(game.get_valid_action_mask(game.players[0].hand, M,
                                                 game.active_level, game.last_play),
                        dtype=torch.float32).unsqueeze(0).to(device) #è·å–æœ‰æ•ˆåŠ¨ä½œ
                    mask = mask.squeeze(0) 
                    actor.eval()
                    with torch.no_grad():
                        probs, _ = actor(state_tensor, mask)
                        temperature = 1.1
                        adj_probs = (probs ** (1/temperature))
                        adj_probs = adj_probs / adj_probs.sum()
                        dist = Categorical(adj_probs)
                        action = dist.sample()
                        log_prob = dist.log_prob(action)
                        action_id = action.item()
                    actor.train()
                    entry = M_id_dict[action_id]
                    player = game.players[0]
                    hand_size_before = len(player.hand)
                    # ç©å®¶çš„åŠ¨ä½œé€šè¿‡ enumerate_colorful_actions å‡½æ•°ç”Ÿæˆå¯èƒ½çš„ç»„åˆã€‚
                    # å¦‚æœæœ‰å¯é€‰åŠ¨ä½œï¼Œä»£ç éšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œå¹¶æ›´æ–°æ¸¸æˆçŠ¶æ€ï¼ŒåŒ…æ‹¬ç©å®¶æ‰‹ç‰Œã€æœ€è¿‘åŠ¨ä½œè®°å½•ä»¥åŠæ¸¸æˆæ—¥å¿—ã€‚
                    # å¦‚æœç©å®¶å‡ºå®Œæ‰€æœ‰ç‰Œï¼Œä»£ç æ›´æ–°æ’åå¹¶æ£€æŸ¥æ¸¸æˆæ˜¯å¦ç»“æŸã€‚
                    combos = enumerate_colorful_actions(entry, player.hand, game.active_level) 
                    if combos:
                        chosen_move = random.choice(combos)
                        if not chosen_move:
                            game.log(f"ç©å®¶ 1 Pass")
                            game.pass_count += 1
                            game.recent_actions[0] = ['Pass']
                        else:
                            game.last_play = chosen_move
                            game.last_player = 0
                            for card in chosen_move:
                                player.played_cards.append(card)
                                player.hand.remove(card)
                            game.log(f"ç©å®¶ 1 å‡ºç‰Œ: {' '.join(chosen_move)}")
                            game.recent_actions[0] = list(chosen_move)
                            game.jiefeng = False
                            if not player.hand:
                                game.log(f"\nğŸ‰ ç©å®¶ 1 å‡ºå®Œæ‰€æœ‰ç‰Œï¼\n")
                                game.ranking.append(0)  # ç©å®¶0çš„ç´¢å¼•ç›´æ¥ä½¿ç”¨0
                                # ç«‹å³ç»“æŸå½“å‰ç©å®¶å›åˆ
                                game.is_game_over = len(game.ranking) >= 4
                                game.pass_count = 0
                                break  # è·³å‡ºå½“å‰å›åˆå¾ªç¯
                                
                            game.pass_count = 0
                            # ç§»é™¤é‡å¤çš„handæ£€æŸ¥
                            if game.is_free_turn:
                                game.is_free_turn = False
                    else:
                        game.log(f"ç©å®¶ 1 Pass")
                        game.pass_count += 1
                        game.recent_actions[0] = ['Pass']
                    next_state = game._get_obs()
                    reward = calculate_improved_reward(entry, player, mask, action_id, 
                                                    hand_size_before, game, ep)
                    episode_reward += reward
                    memory.push(state, action_id, reward, next_state, game.is_game_over, log_prob.item())
                    player.last_played_cards = game.recent_actions[0]
                    game.current_player = (game.current_player + 1) % 4 
                    episode_steps += 1
                else:
                    # æ·»åŠ AIç©å®¶ç»“æŸæ£€æŸ¥
                    current_ai_player = game.players[game.current_player]
                    game.ai_play(current_ai_player)
                    
                    # æ£€æŸ¥AIç©å®¶æ˜¯å¦å‡ºå®Œç‰Œï¼ˆä½¿ç”¨ç©å®¶ç´¢å¼•ä»£æ›¿seatå±æ€§ï¼‰
                    if not current_ai_player.hand and game.current_player not in game.ranking:
                        game.ranking.append(game.current_player)
                        game.is_game_over = len(game.ranking) >= 4
                        if game.is_game_over:
                            break  # ç«‹å³ç»“æŸæ¸¸æˆå¾ªç¯
                        
                round_history = []
                if game.current_player == 0 and any(action != ['None'] for action in game.recent_actions):
                    round_history = [action.copy() for action in game.recent_actions]
                    game.history.append(round_history) 
                    game.recent_actions = [['None'] for _ in range(4)]
                
                all_others_pass = False    
                if len(round_history) == 4 and round_history[0] != ['Pass']:
                    all_others_pass = all(action == ['Pass'] for action in round_history[1:4])
                    
                if all_others_pass:
                    game.is_free_turn = True
                    game.last_play = []
                    game.pass_count = 0
                    if hasattr(game, 'last_player'):
                        game.current_player = game.last_player
                    print("è‡ªç”±å‡ºç‰Œè½®ï¼Œmask:", mask.cpu().numpy())

                if all(action == ['Pass'] for action in game.recent_actions):
                    game.is_free_turn = True
                    game.last_play = []
                    game.pass_count = 0
                    print("æ­»å¾ªç¯æ£€æµ‹ï¼Œmask:", mask.cpu().numpy())
        
                # åŠ¨æ€è°ƒæ•´batch size
                if ep % adaptive_params['batch_growth_interval'] == 0:
                    adaptive_params['current_batch_size'] = min(
                        adaptive_params['max_batch_size'],
                        adaptive_params['current_batch_size'] + 32
                    )
                
                if len(memory) >= adaptive_params['current_batch_size']:
                    states, actions, rewards, next_states, dones, old_log_probs = memory.sample(
                        adaptive_params['current_batch_size']
                    )
                    # æ·»åŠ è®­ç»ƒæ­¥éª¤å¹¶æ¥æ”¶è¿”å›å€¼
                    policy_loss, value_loss, entropy, kl_div = train_on_batch_ppo(
                        states, actions, rewards, next_states, dones, old_log_probs,
                        backbone, actor, critic, target_critic, optimizer,
                        gamma=0.99, gae_lambda=0.97, device=device, ep=ep
                    )
                    
                soft_update(target_backbone, backbone)
                soft_update(target_critic, critic)
                scheduler.step(policy_loss)
                writer.add_scalar('Training/PolicyLoss', policy_loss, ep)
                writer.add_scalar('Training/ValueLoss', value_loss, ep)
                writer.add_scalar('Training/Entropy', entropy, ep)
                writer.add_scalar('Training/KLDivergence', kl_div, ep)
                writer.add_scalar('Training/EpisodeReward', episode_reward, ep)
                writer.add_scalar('Training/EpisodeSteps', episode_steps, ep)
                for i, param_group in enumerate(optimizer.param_groups):
                    writer.add_scalar(f'Training/LR_group_{i}', param_group['lr'], ep)
                    
            if (ep + 1) % 10 == 0:
                # æ·»åŠ æ¢¯åº¦èŒƒæ•°ç›‘æ§
                grad_norms = []
                for param in backbone.parameters():
                    if param.grad is not None:
                        grad_norms.append(param.grad.norm().item())
                avg_grad_norm = sum(grad_norms)/len(grad_norms) if grad_norms else 0
                
                # æ·»åŠ å­¦ä¹ ç‡ç›‘æ§
                lrs = [group['lr'] for group in optimizer.param_groups]
                
                logging.info(
                    f"Episode {ep + 1}: "
                    f"PLoss={policy_loss:.4f}, VLoss={value_loss:.4f}, "
                    f"Entropy={entropy:.4f}, KL={kl_div:.4f}, "
                    f"Reward={episode_reward:.2f}, Steps={episode_steps}, "
                    f"BatchSize={adaptive_params['current_batch_size']}"
                )
                    
            if episode_reward > best_reward:
                best_reward = episode_reward
                save_checkpoint(backbone, actor, critic, optimizer, ep + 1,model_dir="models/best")
                
            if (ep + 1) % 200 == 0:
                save_checkpoint(backbone, actor, critic, optimizer, ep + 1)
                
    except KeyboardInterrupt:
        logging.info("è®­ç»ƒè¢«æ‰‹åŠ¨ä¸­æ–­")
        save_checkpoint(backbone, actor, critic, optimizer, ep + 1,
                      model_dir="models/interrupted")
    finally:
        writer.close()

if __name__ == "__main__":
    logging.info("å¼€å§‹è®­ç»ƒ æ¼è›‹ PPO æ™ºèƒ½ä½“")
    logging.info(f"Pythonç‰ˆæœ¬: {sys.version}")
    logging.info(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    logging.info(f"è®¾å¤‡: {device}")
    run_training()
