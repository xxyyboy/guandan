"""
æ”¹è¿›çš„PPOç®—æ³•å®ç° - æ¼è›‹çº¸ç‰Œæ¸¸æˆAIè®­ç»ƒ 20250609
æ ¸å¿ƒç›®æ ‡ï¼šé™ä½AIé€‰æ‹©Passçš„æ¦‚ç‡ï¼Œé¼“åŠ±ç§¯æå‡ºç‰Œ
"""

import os
import sys
import math
import numpy as np
from pathlib import Path
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical
import json
from guandan_env import GuandanGame
from get_actions import enumerate_colorful_actions
import random
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime
import logging
from copy import deepcopy

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('training.log'),
        logging.StreamHandler()
    ]
)

# åŠ è½½åŠ¨ä½œé…ç½®
try:
    with open("guandan_actions.json", "r", encoding="utf-8") as f:
        M = json.load(f)
    action_dim = len(M)
    M_id_dict = {a['id']: a for a in M}
except FileNotFoundError:
    logging.error("æœªæ‰¾åˆ°guandan_actions.jsonæ–‡ä»¶")
    raise

RANKS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logging.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

class ReplayBuffer:
    """å­˜å‚¨è®­ç»ƒæ ·æœ¬çš„å¾ªç¯ç¼“å†²åŒº"""
    def __init__(self, capacity=10000):
        self.buffer = []  # å­˜å‚¨æ ·æœ¬çš„åˆ—è¡¨
        self.capacity = capacity # æœ€å¤§å®¹é‡
        self.position = 0
        
    def push(self, state, action, reward, next_state, done, log_prob):
        """æ·»åŠ æ–°æ ·æœ¬"""
        if len(self.buffer) < self.capacity:
            self.buffer.append(None)
        self.buffer[self.position] = (state, action, reward, next_state, done, log_prob)
        self.position = (self.position + 1) % self.capacity
        
    def sample(self, batch_size):
        """éšæœºé‡‡æ ·ä¸€æ‰¹æ ·æœ¬"""
        batch = random.sample(self.buffer, min(batch_size, len(self.buffer)))
        states, actions, rewards, next_states, dones, log_probs = map(np.stack, zip(*batch))
        return states, actions, rewards, next_states, dones, log_probs
        
    def __len__(self):
        return len(self.buffer)

class ResidualBlock(nn.Module):
    """æ®‹å·®å—ç»“æ„ï¼Œç¼“è§£æ·±å±‚ç½‘ç»œæ¢¯åº¦æ¶ˆå¤±"""
    def __init__(self, in_dim, out_dim, dropout_rate=0.1):
        super().__init__()
        self.fc1 = nn.Linear(in_dim, out_dim)
        self.bn1 = nn.BatchNorm1d(out_dim)
        self.ln1 = nn.LayerNorm(out_dim)
        self.fc2 = nn.Linear(out_dim, out_dim)
        self.bn2 = nn.BatchNorm1d(out_dim)
        self.ln2 = nn.LayerNorm(out_dim)
        self.dropout = nn.Dropout(dropout_rate)
        self.shortcut = nn.Sequential() # æ·å¾„è¿æ¥
        
        if in_dim != out_dim: # ç»´åº¦ä¸åŒ¹é…æ—¶ä½¿ç”¨æŠ•å½±
            self.shortcut = nn.Sequential(
                nn.Linear(in_dim, out_dim),
                nn.BatchNorm1d(out_dim),
                nn.LayerNorm(out_dim)
            )
            
    def forward(self, x):
        residual = self.shortcut(x)
        out = F.leaky_relu(self.ln1(self.bn1(self.fc1(x))), 0.1)
        out = self.dropout(out)
        out = self.ln2(self.bn2(self.fc2(out)))
        out += residual
        return F.leaky_relu(out, 0.1)

class SharedBackbone(nn.Module):
    """å…±äº«ç‰¹å¾æå–ç½‘ç»œ"""
    def __init__(self, state_dim=3049, hidden_dim=2048):  # å¢å¤§éšè—å±‚ç»´åº¦
        super().__init__()
        # æ‰‹ç‰Œç‰¹å¾ç¼–ç å™¨ (108ç»´)
        self.card_encoder = nn.Sequential(
            nn.Linear(108, 512),
            nn.LeakyReLU(0.1),
            nn.LayerNorm(512)
        )
        
        # å†å²åŠ¨ä½œæ—¶åºç¼–ç å™¨ (2160ç»´)
        self.history_encoder = nn.LSTM(
            input_size=2160, 
            hidden_size=512,
            num_layers=2,
            batch_first=True
        )
        
        # çº§ç‰Œç‰¹å¾åµŒå…¥ (13ç»´)
        self.level_embed = nn.Embedding(13, 64)
        
        # èåˆå±‚
        self.fusion = nn.Sequential(
            nn.Linear(512+512+64, hidden_dim),
            nn.LeakyReLU(0.1),
            nn.LayerNorm(hidden_dim)
        )
        
        # æ®‹å·®å—
        self.res_blocks = nn.ModuleList([
            ResidualBlock(hidden_dim, hidden_dim),
            ResidualBlock(hidden_dim, hidden_dim),
            ResidualBlock(hidden_dim, hidden_dim//2)
        ])
        self.eval_mode = False  # åˆå§‹åŒ–eval_modeå±æ€§
        self._init_weights() # æ­£äº¤åˆå§‹åŒ–
        
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.orthogonal_(m.weight, gain=1.0)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.0)
                    
    def forward(self, x):
        # æ‰‹ç‰Œç‰¹å¾æå–
        card_feat = self.card_encoder(x[..., :108])
        
        # å†å²åŠ¨ä½œç‰¹å¾æå–
        history_feat, _ = self.history_encoder(x[..., 108:2268].unsqueeze(1))
        history_feat = history_feat.squeeze(1)
        
        # çº§ç‰Œç‰¹å¾æå–
        level_idx = torch.argmax(x[..., 2268:2281], dim=-1).long()
        level_feat = self.level_embed(level_idx)
        
        # ç‰¹å¾èåˆ
        x = torch.cat([card_feat, history_feat, level_feat], dim=-1)
        x = self.fusion(x)
        
        # æ®‹å·®å—å¤„ç†
        for res_block in self.res_blocks:
            x = res_block(x)
        return x
    
    def eval(self):
        """è®¾ç½®è¯„ä¼°æ¨¡å¼"""
        super().eval()
        self.eval_mode = True
        
    def train(self, mode=True):
        """è®¾ç½®è®­ç»ƒæ¨¡å¼"""
        super().train(mode)
        self.eval_mode = not mode
        
class ImprovedResNetActor(nn.Module):
    """ç­–ç•¥ç½‘ç»œ"""
    def __init__(self, backbone, action_dim=action_dim):
        super().__init__()
        self.backbone = backbone # å…±äº«ç‰¹å¾æå–
        backbone_out_dim = backbone.res_blocks[-1].fc2.out_features
        # ç­–ç•¥å¤´
        self.fc_policy = nn.Sequential(
            nn.Linear(backbone_out_dim, backbone_out_dim//2),
            nn.LayerNorm(backbone_out_dim//2),
            nn.ReLU(),
            nn.Linear(backbone_out_dim//2, action_dim),
            nn.LayerNorm(action_dim)
        )
        self._init_weights()
        
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.orthogonal_(m.weight, gain=1.0)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.0)
                    
    def forward(self, x, mask=None):
        features = self.backbone(x)
        logits = self.fc_policy(features)
        # é‡ç‚¹ï¼šé™ä½Passï¼ˆå‡è®¾ä¸ºåŠ¨ä½œ0ï¼‰çš„logitåˆ†æ•°ï¼Œè®­ç»ƒåˆæœŸå‡ºç‰Œæ›´ç§¯æ
        logits[..., 0] -= 1.5
        if mask is not None:
            logits = logits + (mask.float() - 1) * 1e9
        probs = F.softmax(logits, dim=-1)
        return probs, logits

class ImprovedResNetCritic(nn.Module):
    """ä»·å€¼ç½‘ç»œ"""
    def __init__(self, backbone):
        super().__init__()
        self.backbone = backbone
        backbone_out_dim = backbone.res_blocks[-1].fc2.out_features
        self.fc_value = nn.Sequential(
            nn.Linear(backbone_out_dim, backbone_out_dim//2),
            nn.LayerNorm(backbone_out_dim//2),
            nn.ReLU(),
            nn.Linear(backbone_out_dim//2, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Tanh() # è¾“å‡ºåœ¨[-1,1]èŒƒå›´
        )
        self._init_weights()
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.orthogonal_(m.weight, gain=1.0)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.0)
    def forward(self, x):
        features = self.backbone(x)
        value = self.fc_value(features)
        return value

def select_action(actor, state, mask, device, is_free_turn, ep):
    """æ™ºèƒ½åŠ¨ä½œé€‰æ‹©ç­–ç•¥3.0"""
    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)
    mask_tensor = torch.tensor(mask, dtype=torch.float32).unsqueeze(0).to(device)
    
    # åŠ¨æ€æ¢ç´¢å‚æ•°
    explore_factor = max(0.1, 0.5 * (0.98 ** (ep // 100)))  # è¡°å‡æ›´æ…¢
    temp = max(0.5, 1.5 - ep/4000)  # æ¸©åº¦å‚æ•°
    
    with torch.no_grad():
        probs, logits = actor(state_tensor, mask_tensor)
        
        # å¼ºåŒ–PassæŠ‘åˆ¶
        if is_free_turn:
            logits[0, 0] = -float('inf')  # è‡ªç”±å‡ºç‰Œç¦æ­¢Pass
        else:
            # åŠ¨æ€Passæƒ©ç½š = åŸºç¡€å€¼ + è®­ç»ƒè¿›åº¦è°ƒæ•´
            pass_penalty = 1.5 + (1 - ep/6000)
            logits[0, 0] -= pass_penalty
            
        # ç‚¸å¼¹åŠ¨ä½œå¥–åŠ±
        bomb_mask = torch.zeros_like(logits)
        bomb_mask[:, 120:456] = 1  # ç‚¸å¼¹åŠ¨ä½œåŒºé—´
        logits += bomb_mask * (0.5 + ep/8000)  # éšè®­ç»ƒé€æ¸é‡è§†ç‚¸å¼¹
        
        # æ¢ç´¢æœºåˆ¶
        if random.random() < explore_factor:
            # ä¼˜å…ˆæ¢ç´¢éPassåŠ¨ä½œ
            valid_actions = torch.nonzero(mask_tensor[0] * (torch.arange(mask_tensor.size(1)) != 0))
            if len(valid_actions) > 0:
                action = valid_actions[torch.randint(0, len(valid_actions), (1,))]
                return action.item()

def validate_game_state(game):
    """éªŒè¯æ¸¸æˆçŠ¶æ€çš„åˆæ³•æ€§"""
    # æ£€æŸ¥å‡ºç‰Œæƒ
    if game.is_free_turn and game.last_play:
        logging.warning("çŠ¶æ€é”™è¯¯ï¼šè‡ªç”±å‡ºç‰Œå›åˆå­˜åœ¨last_play")
        game.last_play = []
        
    # æ£€æŸ¥æ’å
    if len(set(game.ranking)) != len(game.ranking):
        logging.error("çŠ¶æ€é”™è¯¯ï¼šæ’åé‡å¤")
        game.ranking = list(dict.fromkeys(game.ranking))
        
    # æ£€æŸ¥Passè®¡æ•°
    if game.pass_count > 4:
        logging.warning("çŠ¶æ€é”™è¯¯ï¼špass_countè¶…è¿‡4")
        game.pass_count = 0
        
    return True

def calculate_improved_reward(entry, player, mask, action_id, hand_size_before, game, ep):
    """åŠ¨æ€å¥–åŠ±å‡½æ•°2.0 - å¼ºåŒ–å…³é”®å†³ç­–å¥–åŠ±"""
    reward = 0.0
    hand_size = len(player.hand)
    progress = (27 - hand_size) / 27  # æ¸¸æˆè¿›åº¦[0,1]
    
    # 1. æ™ºèƒ½Passæƒ©ç½šæœºåˆ¶
    if action_id == 0:  # PassåŠ¨ä½œ
        # åŠ¨æ€æƒ©ç½šç³»æ•° = åŸºç¡€æƒ©ç½š + è¿›åº¦æƒ©ç½š + è®­ç»ƒé˜¶æ®µè¡°å‡
        base_penalty = 0.8 + (1 - progress)**2  # åæœŸæƒ©ç½šåŠ é‡
        phase_factor = max(0.3, 1.0 - ep/5000)  # è®­ç»ƒåæœŸé™ä½æƒ©ç½š
        penalty = -base_penalty * phase_factor
        
        # è‡ªç”±å‡ºç‰Œå›åˆç¦æ­¢Pass
        if game.is_free_turn:
            penalty *= 2.0
            
        # å¯¹æ‰‹å³å°†å‡ºå®Œç‰Œæ—¶åŠ é‡æƒ©ç½š
        opp_hands = [len(p.hand) for i,p in enumerate(game.players) if i != 0]
        if min(opp_hands) <= 3:
            penalty *= 1.5 + (4 - min(opp_hands))*0.3
            
        reward += penalty
        
    # 2. å‡ºç‰Œå¥–åŠ±ä½“ç³»
    else:
        # åŸºç¡€å¥–åŠ± (åŠ¨æ€è°ƒæ•´)
        base_reward = 0.5 + progress*0.5  # åæœŸå¥–åŠ±æ›´é«˜
        
        # ç‰Œå‹ç³»æ•°
        action_type = entry.get('type', '')
        type_bonus = {
            'single': 0.1, 'pair': 0.2, 'trio': 0.3,
            'bomb': 1.2, 'rocket': 2.0, 'sequence': 0.4
        }.get(action_type, 0.0)
        
        # æ•°é‡å¥–åŠ± (éçº¿æ€§å¢é•¿)
        cards_played = hand_size_before - hand_size
        count_bonus = 0.2 * math.sqrt(cards_played)
        
        reward += base_reward + type_bonus + count_bonus
        
        # å…³é”®é˜¶æ®µå¥–åŠ± (å‰©ä½™ç‰Œ<5æ—¶æŒ‡æ•°å¢é•¿)
        if hand_size <= 5:
            reward += (6 - hand_size)**1.5 * 0.2
            
    # 3. ç»ˆå±€å¥–åŠ±ä¼˜åŒ–
    if game.is_game_over:
        rank = game.ranking.index(0)  # ç©å®¶åæ¬¡
        rank_rewards = [10.0, 7.0, -4.0, -6.0]  # æ›´å¹³ç¼“çš„å¥–åŠ±æ›²çº¿
        
        # é˜Ÿä¼èƒœåˆ©æ£€æµ‹
        teammate = 2 if 0 in [0,2] else 3
        if rank < 2 and teammate in game.ranking[:2]:
            rank_rewards[rank] *= 1.5  # é˜Ÿä¼èƒœåˆ©åŠ æˆ
            
        reward += rank_rewards[rank]
    
    return reward  # ç¡®ä¿å‡½æ•°æœ‰è¿”å›å€¼

def compute_gae(rewards, values, next_values, dones, gamma=0.99, gae_lambda=0.95):
    batch_size = len(rewards)
    advantages = torch.zeros_like(rewards)
    gae = 0
    for t in reversed(range(batch_size)):
        if t == batch_size - 1:
            next_value = next_values[t]
        else:
            next_value = values[t + 1]
        delta = rewards[t] + gamma * next_value * (1 - dones[t].float()) - values[t]
        gae = delta + gamma * gae_lambda * (1 - dones[t].float()) * gae
        advantages[t] = gae
    returns = advantages + values
    # æ·»åŠ ä¼˜åŠ¿å€¼è£å‰ªå’Œæ›´ç¨³å®šçš„æ ‡å‡†åŒ–
    advantages = torch.clamp(advantages, -3.0, 3.0)
    adv_mean = advantages.mean()
    adv_std = advantages.std(unbiased=False) + 1e-7
    advantages = (advantages - adv_mean) / adv_std
    return advantages, returns

def save_checkpoint(backbone, actor, critic, optimizer, ep, model_dir="models"):
    os.makedirs(model_dir, exist_ok=True)
    checkpoint = {
        'backbone_state_dict': backbone.state_dict(),
        'actor_state_dict': actor.state_dict(),
        'critic_state_dict': critic.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'episode': ep
    }
    torch.save(checkpoint, f"{model_dir}/checkpoint_ep{ep}.pth")
    logging.info(f"ä¿å­˜æ£€æŸ¥ç‚¹: checkpoint_ep{ep}.pth")

def load_checkpoint(device, backbone, actor, critic, optimizer, model_dir="models"):
    model_files = sorted(Path(model_dir).glob("checkpoint_ep*.pth"))
    if not model_files:
        logging.info("æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
        return 0
    latest_checkpoint = str(model_files[-1])
    checkpoint = torch.load(latest_checkpoint, map_location=device)
    backbone.load_state_dict(checkpoint['backbone_state_dict'])
    actor.load_state_dict(checkpoint['actor_state_dict'])
    critic.load_state_dict(checkpoint['critic_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    ep = checkpoint['episode']
    logging.info(f"åŠ è½½æ£€æŸ¥ç‚¹: {latest_checkpoint}")
    return ep


def train_on_batch_ppo(states, actions, rewards, next_states, dones, old_log_probs,
                      backbone, actor, critic, target_critic, optimizer,
                      gamma=0.99, gae_lambda=0.95, clip_epsilon=0.2,
                      entropy_coef=0.1, device=device, ep=0):
    states = torch.FloatTensor(states).to(device)
    next_states = torch.FloatTensor(next_states).to(device)
    actions = torch.LongTensor(actions).to(device)
    rewards = torch.FloatTensor(rewards).to(device)
    dones = torch.BoolTensor(dones).to(device)
    old_log_probs = torch.FloatTensor(old_log_probs).to(device)
    with torch.no_grad():
        next_values = target_critic(next_states).squeeze(-1)
    values = critic(states).squeeze(-1)
    advantages, returns = compute_gae(rewards, values, next_values, dones, gamma, gae_lambda)
    probs, logits = actor(states)
    dist = Categorical(probs)
    new_log_probs = dist.log_prob(actions)
    entropy = dist.entropy().mean()
    
    # è°ƒæ•´ç†µç³»æ•°
    if ep < 200:
        entropy_coef = 0.05  # é™ä½åˆå§‹ç†µç³»æ•°
    else:
        entropy_coef = max(0.005, 0.05 * (0.995 ** ((ep-200)//50)))
        
    kl_div = (old_log_probs - new_log_probs).mean()
    
    ratios = torch.exp(new_log_probs - old_log_probs)
    surr1 = ratios * advantages
    surr2 = torch.clamp(ratios, 1.0 - clip_epsilon, 1.0 + clip_epsilon) * advantages
    policy_loss = -torch.min(surr1, surr2).mean()
    # ç­–ç•¥æ­£åˆ™åŒ–å’ŒPassæƒ©ç½š
    pass_probs = probs[:, 0]
    policy_loss += 0.1 * pass_probs.mean()
    if kl_div > 0.02:
        policy_loss *= 1.2
    value_clipped = values + (returns - values).clamp(-clip_epsilon, clip_epsilon)
    critic_loss1 = F.smooth_l1_loss(values, returns)
    critic_loss2 = F.smooth_l1_loss(value_clipped, returns)
    critic_loss = torch.max(critic_loss1, critic_loss2).mean()
    value_loss_weight = max(0.25, 0.5 * (0.999 ** ep))
    total_loss = policy_loss + value_loss_weight * critic_loss - entropy_coef * entropy
    optimizer.zero_grad()
    total_loss.backward()
    torch.nn.utils.clip_grad_norm_(backbone.parameters(), max_norm=1.0)
    torch.nn.utils.clip_grad_norm_(actor.parameters(), max_norm=1.0)
    torch.nn.utils.clip_grad_norm_(critic.parameters(), max_norm=1.0)
    optimizer.step()
    return policy_loss.item(), critic_loss.item(), entropy.item(), kl_div.item()

# ...å…¶ä½™å†…å®¹ä¸å˜...

def run_training(episodes=30000):
    """æ”¹è¿›çš„è®­ç»ƒæµç¨‹ - æ·»åŠ è¯¾ç¨‹å­¦ä¹ å’Œç›®æ ‡ç½‘ç»œ"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # è¯¾ç¨‹å­¦ä¹ é…ç½®
    curriculum = {
        0: {'level': (2,5), 'opponent': 'random'},
        5000: {'level': (5,8), 'opponent': 'rule_based'},
        10000: {'level': (8,14), 'opponent': 'self'}
    }
    
    # åˆå§‹åŒ–ç½‘ç»œ
    backbone = SharedBackbone().to(device)
    actor = ImprovedResNetActor(backbone).to(device)
    critic = ImprovedResNetCritic(backbone).to(device)
    
    # åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œï¼ˆç”¨äºç¨³å®šè®­ç»ƒï¼‰
    target_backbone = SharedBackbone().to(device)
    target_critic = ImprovedResNetCritic(target_backbone).to(device)
    
    # åŒæ­¥åˆå§‹å‚æ•°
    target_backbone.load_state_dict(backbone.state_dict())
    target_critic.load_state_dict(critic.state_dict())
    
    # ä¼˜åŒ–å™¨åˆ†ç»„è®¾ç½®
    optimizer_params = []
    
    # 1. æ·»åŠ actorç‹¬æœ‰å‚æ•°
    optimizer_params.append({
        'params': [p for n,p in actor.named_parameters() 
                  if not n.startswith('backbone.')],
        'lr': 3e-5
    })
    
    # 2. æ·»åŠ criticç‹¬æœ‰å‚æ•°
    optimizer_params.append({
        'params': [p for n,p in critic.named_parameters()
                  if not n.startswith('backbone.')],
        'lr': 3e-5
    })
    
    # 3. æ·»åŠ å…±äº«ä¸»å¹²å‚æ•°ï¼ˆåªæ·»åŠ ä¸€æ¬¡ï¼‰
    optimizer_params.append({
        'params': backbone.parameters(),
        'lr': 1e-5
    })
    
    optimizer = optim.AdamW(optimizer_params, weight_decay=1e-4)
    
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, 'min', patience=50, factor=0.97, min_lr=1e-5
    )
    memory = ReplayBuffer(capacity=10000)
    writer = SummaryWriter(f'runs/guandan_ppo_{datetime.now().strftime("%Y%m%d_%H%M%S")}')
    initial_ep = load_checkpoint(device, backbone, actor, critic, optimizer)
    best_reward = float('-inf')
    def soft_update(target, source, tau=0.001):
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(
                target_param.data * (1.0 - tau) + param.data * tau
            )
    try:
        for ep in range(initial_ep, initial_ep + episodes):
            game = GuandanGame(verbose=True)
            episode_reward = 0
            episode_steps = 0
            pass_penalty_given = False  # é˜²æ­¢å¤šæ¬¡æƒ©ç½š
            while not game.is_game_over and len(game.history) <= 200:
                # å¼ºåŒ–è¿ç»­Passæ£€æµ‹å’Œå¤„ç†
                if game.pass_count >= 2:  # é™ä½æ£€æµ‹é˜ˆå€¼
                    # è®°å½•è¿ç»­Passè½®æ¬¡
                    consecutive_pass_rounds = game.pass_count
                    
                    # é‡ç½®å‡ºç‰Œæƒç»™æœ€åå‡ºç‰Œçš„ç©å®¶
                    if game.last_player is not None:
                        game.current_player = game.last_player
                        game.log(f"ç©å®¶ {game.last_player + 1} è·å¾—æ–°ä¸€è½®å‡ºç‰Œæƒï¼ˆè¿ç»­{consecutive_pass_rounds}æ¬¡Passåï¼‰")
                    
                    # åº”ç”¨å…¨å±€æƒ©ç½šï¼ˆæ¯è½®Passéƒ½æƒ©ç½šï¼‰
                    global_penalty = -0.5 * consecutive_pass_rounds
                    if len(memory) > 0:
                        memory.push(memory.buffer[-1][0], 0, global_penalty, 
                                  memory.buffer[-1][0], False, 0.0)
                        episode_reward += global_penalty
                    
                    # é‡ç½®çŠ¶æ€
                    game.last_play = []
                    game.pass_count = 0
                    game.recent_actions = [['None'] for _ in range(4)]
                    continue
    
                # æ£€æŸ¥è¿ç»­4æ¬¡Passæƒ©ç½š
                if game.pass_count >= 4 and not pass_penalty_given:
                    # å¯¹æ‰€æœ‰ç©å®¶ï¼ˆè¿™é‡Œåªè®­ç»ƒä¸»æ™ºèƒ½ä½“ï¼Œrewardè®°å½•åœ¨memoryï¼‰
                    penalty = -2.0
                    # ä»…å¯¹ä¸»æ™ºèƒ½ä½“åšè®°å½•
                    if len(memory) > 0:
                        memory.push(memory.buffer[-1][0], 0, penalty, memory.buffer[-1][0], False, 0.0)
                        episode_reward += penalty
                    # æ—¥å¿—æç¤º
                    logging.info("æ‰€æœ‰ç©å®¶è¿ç»­4æ¬¡Passï¼Œç»™äºˆæ‰€æœ‰ç©å®¶æƒ©ç½šï¼")
                    # æ¸…ç©ºpass_countå’Œrecent_actionsï¼Œè¿›å…¥æ–°ä¸€è½®
                    game.pass_count = 0
                    game.recent_actions = [['None'] for _ in range(4)]
                    pass_penalty_given = True
                else:
                    pass_penalty_given = False

                if game.current_player == 0:
                    state = game._get_obs()
                    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)
                    mask = torch.tensor(game.get_valid_action_mask(game.players[0].hand, M,
                                                 game.active_level, game.last_play),
                        dtype=torch.float32).unsqueeze(0).to(device)
                    mask = mask.squeeze(0)
                    actor.eval()
                    with torch.no_grad():
                        probs, _ = actor(state_tensor, mask)
                        temperature = 1.1
                        adj_probs = (probs ** (1/temperature))
                        adj_probs = adj_probs / adj_probs.sum()
                        dist = Categorical(adj_probs)
                        action = dist.sample()
                        log_prob = dist.log_prob(action)
                        action_id = action.item()
                    actor.train()
                    entry = M_id_dict[action_id]
                    player = game.players[0]
                    hand_size_before = len(player.hand)
                    combos = enumerate_colorful_actions(entry, player.hand, game.active_level)
                    if combos:
                        chosen_move = random.choice(combos)
                        if not chosen_move:
                            game.log(f"ç©å®¶ 1 Pass")
                            game.pass_count += 1
                            game.recent_actions[0] = ['Pass']
                        else:
                            game.last_play = chosen_move
                            game.last_player = 0
                            for card in chosen_move:
                                player.played_cards.append(card)
                                player.hand.remove(card)
                            game.log(f"ç©å®¶ 1 å‡ºç‰Œ: {' '.join(chosen_move)}")
                            game.recent_actions[0] = list(chosen_move)
                            game.jiefeng = False
                            if not player.hand:
                                game.log(f"\nğŸ‰ ç©å®¶ 1 å‡ºå®Œæ‰€æœ‰ç‰Œï¼\n")
                                game.ranking.append(0)  # ç©å®¶0çš„ç´¢å¼•ç›´æ¥ä½¿ç”¨0
                                # ç«‹å³ç»“æŸå½“å‰ç©å®¶å›åˆ
                                game.is_game_over = len(game.ranking) >= 4
                                game.pass_count = 0
                                break  # è·³å‡ºå½“å‰å›åˆå¾ªç¯
                                
                            game.pass_count = 0
                            # ç§»é™¤é‡å¤çš„handæ£€æŸ¥
                            if game.is_free_turn:
                                game.is_free_turn = False
                    else:
                        game.log(f"ç©å®¶ 1 Pass")
                        game.pass_count += 1
                        game.recent_actions[0] = ['Pass']
                    next_state = game._get_obs()
                    reward = calculate_improved_reward(entry, player, mask, action_id, 
                                                    hand_size_before, game, ep)
                    episode_reward += reward
                    memory.push(state, action_id, reward, next_state, game.is_game_over, log_prob.item())
                    player.last_played_cards = game.recent_actions[0]
                    game.current_player = (game.current_player + 1) % 4 
                    episode_steps += 1
                else:
                    # æ·»åŠ AIç©å®¶ç»“æŸæ£€æŸ¥
                    current_ai_player = game.players[game.current_player]
                    game.ai_play(current_ai_player)
                    
                    # æ£€æŸ¥AIç©å®¶æ˜¯å¦å‡ºå®Œç‰Œï¼ˆä½¿ç”¨ç©å®¶ç´¢å¼•ä»£æ›¿seatå±æ€§ï¼‰
                    if not current_ai_player.hand and game.current_player not in game.ranking:
                        game.ranking.append(game.current_player)
                        game.is_game_over = len(game.ranking) >= 4
                        if game.is_game_over:
                            break  # ç«‹å³ç»“æŸæ¸¸æˆå¾ªç¯
                round_history = []
                if game.current_player == 0 and any(action != ['None'] for action in game.recent_actions):
                    round_history = [action.copy() for action in game.recent_actions]
                    game.history.append(round_history) 
                    game.recent_actions = [['None'] for _ in range(4)]
                
                all_others_pass = False    
                if len(round_history) == 4 and round_history[0] != ['Pass']:
                    all_others_pass = all(action == ['Pass'] for action in round_history[1:4])
                    
                if all_others_pass:
                    game.is_free_turn = True
                    game.last_play = []
                    game.pass_count = 0
                    if hasattr(game, 'last_player'):
                        game.current_player = game.last_player
                    print("è‡ªç”±å‡ºç‰Œè½®ï¼Œmask:", mask.cpu().numpy())

                if all(action == ['Pass'] for action in game.recent_actions):
                    game.is_free_turn = True
                    game.last_play = []
                    game.pass_count = 0
                    print("æ­»å¾ªç¯æ£€æµ‹ï¼Œmask:", mask.cpu().numpy())
        
            if len(memory) >= 128:
                states, actions, rewards, next_states, dones, old_log_probs = memory.sample(128)
                policy_loss, value_loss, entropy, kl_div = train_on_batch_ppo(
                    states, actions, rewards, next_states, dones, old_log_probs,
                    backbone, actor, critic, target_critic, optimizer,
                    entropy_coef=0.1, device=device, ep=ep
                )
                soft_update(target_backbone, backbone)
                soft_update(target_critic, critic)
                scheduler.step(policy_loss)
                writer.add_scalar('Training/PolicyLoss', policy_loss, ep)
                writer.add_scalar('Training/ValueLoss', value_loss, ep)
                writer.add_scalar('Training/Entropy', entropy, ep)
                writer.add_scalar('Training/KLDivergence', kl_div, ep)
                writer.add_scalar('Training/EpisodeReward', episode_reward, ep)
                writer.add_scalar('Training/EpisodeSteps', episode_steps, ep)
                for i, param_group in enumerate(optimizer.param_groups):
                    writer.add_scalar(f'Training/LR_group_{i}', param_group['lr'], ep)
                if (ep + 1) % 50 == 0:
                    logging.info(
                        f"Episode {ep + 1}: "
                        f"PLoss={policy_loss:.4f}, VLoss={value_loss:.4f}, "
                        f"Entropy={entropy:.4f}, KL={kl_div:.4f}, "
                        f"Reward={episode_reward:.2f}, Steps={episode_steps}"
                    )
                if episode_reward > best_reward:
                    best_reward = episode_reward
                    save_checkpoint(backbone, actor, critic, optimizer, ep + 1,
                                 model_dir="models/best")
            if (ep + 1) % 200 == 0:
                save_checkpoint(backbone, actor, critic, optimizer, ep + 1)
    except KeyboardInterrupt:
        logging.info("è®­ç»ƒè¢«æ‰‹åŠ¨ä¸­æ–­")
        save_checkpoint(backbone, actor, critic, optimizer, ep + 1,
                      model_dir="models/interrupted")
    finally:
        writer.close()

if __name__ == "__main__":
    logging.info("å¼€å§‹è®­ç»ƒ æ¼è›‹ PPO æ™ºèƒ½ä½“")
    logging.info(f"Pythonç‰ˆæœ¬: {sys.version}")
    logging.info(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    logging.info(f"è®¾å¤‡: {device}")
    run_training()
