"""
PPO.py
æ¼è›‹Proximal Policy Optimization (PPO) å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“è®­ç»ƒä»£ç ã€‚
å®ç°PPOæ ¸å¿ƒï¼šç­–ç•¥/ä»·å€¼ç½‘ç»œã€GAEä¼˜åŠ¿ä¼°è®¡ã€æŸå¤±å‡½æ•°è£å‰ªã€ç†µæ­£åˆ™ã€æ–­ç‚¹ç»­è®­ã€é‡‡æ ·ä¸è®­ç»ƒæµç¨‹ã€‚
"""
import os
import math
import numpy as np
from pathlib import Path
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical
import json
from guandan_env import GuandanGame
from get_actions import enumerate_colorful_actions
import random

# åŠ è½½åŠ¨ä½œå…¨é›† M
with open("doudizhu_actions.json", "r", encoding="utf-8") as f:
    M = json.load(f)
action_dim = len(M)
RANKS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']

# æ„å»ºåŠ¨ä½œæ˜ å°„å­—å…¸
M_id_dict = {a['id']: a for a in M}

def find_entry_by_id(data, target_id):
    """è¿”å›åŒ¹é… id çš„æ•´ä¸ª JSON å¯¹è±¡"""
    for entry in data:
        if entry.get("id") == target_id:
            return entry
    return None

# æ®‹å·®å—å®šä¹‰
class ResidualBlock(nn.Module):
    def __init__(self, in_dim, out_dim):
        super().__init__()
        self.fc1 = nn.Linear(in_dim, out_dim)
        self.bn1 = nn.BatchNorm1d(out_dim)
        self.fc2 = nn.Linear(out_dim, out_dim)
        self.bn2 = nn.BatchNorm1d(out_dim)
        self.shortcut = nn.Sequential()
        if in_dim != out_dim:
            self.shortcut = nn.Sequential(
                nn.Linear(in_dim, out_dim),
                nn.BatchNorm1d(out_dim)
            )
            
    def forward(self, x):
        residual = self.shortcut(x)
        out = F.leaky_relu(self.bn1(self.fc1(x)), 0.1)
        out = self.bn2(self.fc2(out))
        out += residual
        return F.leaky_relu(out, 0.1)

# åŸºäºæ®‹å·®ç½‘ç»œçš„Actor
class ResNetActor(nn.Module):
    def __init__(self, state_dim=3049, action_dim=action_dim, hidden_dim=1024):
        super().__init__()
        # è¾“å…¥å±‚
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.bn1 = nn.BatchNorm1d(hidden_dim)
        
        # æ®‹å·®å—ï¼ˆæ·»åŠ Dropoutæ­£åˆ™åŒ–ï¼‰
        self.res1 = ResidualBlock(hidden_dim, hidden_dim)
        self.drop1 = nn.Dropout(0.1)
        self.res2 = ResidualBlock(hidden_dim, hidden_dim//2)
        self.drop2 = nn.Dropout(0.1)
        self.res3 = ResidualBlock(hidden_dim//2, hidden_dim//4)
        self.drop3 = nn.Dropout(0.1)
        
        # è¾“å‡ºå±‚
        self.fc_out = nn.Linear(hidden_dim//4, action_dim)
        nn.init.xavier_uniform_(self.fc_out.weight)
        
    def forward(self, x, mask=None):
        x = F.leaky_relu(self.bn1(self.fc1(x)), 0.1)
        x = self.drop1(self.res1(x))
        x = self.drop2(self.res2(x))
        x = self.drop3(self.res3(x))
        logits = self.fc_out(x)
        if mask is not None:
            # ä½¿ç”¨maskè¿‡æ»¤éæ³•åŠ¨ä½œ
            logits = logits + (mask - 1) * 1e9
        return F.softmax(logits, dim=-1)

# åŸºäºæ®‹å·®ç½‘ç»œçš„Critic
class ResNetCritic(nn.Module):
    def __init__(self, state_dim=3049, hidden_dim=1024):
        super().__init__()
        # è¾“å…¥å±‚
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.bn1 = nn.BatchNorm1d(hidden_dim)
        
        # æ®‹å·®å—ï¼ˆå¢åŠ æ·±åº¦ï¼‰
        self.res1 = ResidualBlock(hidden_dim, hidden_dim)
        self.res2 = ResidualBlock(hidden_dim, hidden_dim)
        self.res3 = ResidualBlock(hidden_dim, hidden_dim//2)
        self.res4 = ResidualBlock(hidden_dim//2, hidden_dim//4)
        
        # ä»·å€¼è¾“å‡ºï¼ˆå¢åŠ å±‚æ•°ï¼‰
        self.value_head = nn.Sequential(
            nn.Linear(hidden_dim//4, hidden_dim//8),
            nn.LeakyReLU(0.1),
            nn.Linear(hidden_dim//8, hidden_dim//16),
            nn.LeakyReLU(0.1),
            nn.Linear(hidden_dim//16, 1)
        )
        # åˆå§‹åŒ–è¾“å‡ºå±‚æƒé‡
        nn.init.xavier_uniform_(self.value_head[-1].weight, gain=0.01)
        nn.init.constant_(self.value_head[-1].bias, 0.0)
        
    def forward(self, x):
        x = F.leaky_relu(self.bn1(self.fc1(x)), 0.1)
        x = self.res1(x)
        x = self.res2(x)
        x = self.res3(x)
        x = self.res4(x)
        value = self.value_head(x)
        return value

# åˆå§‹åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨GPUä¼˜å…ˆï¼‰
#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device="cpu"
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# ä½¿ç”¨æ®‹å·®ç½‘ç»œç»“æ„
actor = ResNetActor().to(device)
critic = ResNetCritic().to(device)

# ä½¿ç”¨å¸¦æƒé‡è¡°å‡çš„ä¼˜åŒ–å™¨
actor_optimizer = optim.AdamW(actor.parameters(), lr=5e-5, weight_decay=1e-5)
critic_optimizer = optim.AdamW(critic.parameters(), lr=5e-5, weight_decay=1e-5)

# å­¦ä¹ ç‡è°ƒåº¦å™¨
actor_scheduler = optim.lr_scheduler.ReduceLROnPlateau(actor_optimizer, 'min', patience=100, factor=0.97, min_lr=1e-5)
critic_scheduler = optim.lr_scheduler.ReduceLROnPlateau(critic_optimizer, 'min', patience=100, factor=0.97, min_lr=1e-5)

gamma = 0.98 # ç•¥å¾®é™ä½æŠ˜æ‰£å› å­ï¼Œå¹³è¡¡å³æ—¶å¥–åŠ±å’Œé•¿æœŸå¥–åŠ±
gae_lambda = 0.92 # é™ä½GAEå¹³æ»‘å‚æ•°ï¼Œå‡å°‘ä¼˜åŠ¿ä¼°è®¡åå·®

# å°è¯•åŠ è½½å·²æœ‰æ¨¡å‹
def load_latest_models(actor, critic, model_dir="models", device=device):
    model_files = sorted(Path(model_dir).glob("actor_ppo_ep*.pth"))
    if model_files:
        latest_actor_path = str(model_files[-1])
        ep = int(latest_actor_path.split("_ep")[1].split(".pth")[0])
        latest_critic_path = f"{model_dir}/critic_ppo_ep{ep}.pth"

        # åŠ è½½æ¨¡å‹åˆ°æŒ‡å®šè®¾å¤‡
        actor.load_state_dict(torch.load(latest_actor_path, map_location=device))
        print(f"âœ… åŠ è½½å·²æœ‰ actor æ¨¡å‹: {latest_actor_path}")

        if Path(latest_critic_path).exists():
            critic.load_state_dict(torch.load(latest_critic_path, map_location=device))
            print(f"âœ… åŠ è½½å·²æœ‰ critic æ¨¡å‹: {latest_critic_path}")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ° critic æ¨¡å‹: {latest_critic_path}")

        return ep
    return 0

# è°ƒç”¨åŠ è½½å‡½æ•°
initial_ep = load_latest_models(actor, critic, device=device)

def compute_gae(rewards, values, dones, gamma=0.99, gae_lambda=0.95):
    """è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡(GAE)
    
    å‚æ•°:
        rewards (Tensor): å¥–åŠ±åºåˆ—
        values (Tensor): çŠ¶æ€ä»·å€¼åºåˆ—
        dones (Tensor): ç»ˆæ­¢çŠ¶æ€æ ‡è®°(å¸ƒå°”å¼ é‡)
        gamma (float): æŠ˜æ‰£å› å­
        gae_lambda (float): GAEå¹³æ»‘å‚æ•°
        
    è¿”å›:
        tuple: (advantages, returns) ä¼˜åŠ¿å‡½æ•°å’Œå›æŠ¥
    """
    advantages = []
    gae = 0
    next_value = 0  # å‡è®¾æœ€åä¸€æ­¥æ— æœªæ¥å›æŠ¥

    # å°†å¸ƒå°”å‹donesè½¬æ¢ä¸ºæµ®ç‚¹å‹(1.0è¡¨ç¤ºç»ˆæ­¢ï¼Œ0.0è¡¨ç¤ºç»§ç»­)
    dones_float = dones.float()
    
    for t in reversed(range(len(rewards))):
        delta = rewards[t] + gamma * next_value * (1 - dones_float[t]) - values[t]
        gae = delta + gamma * gae_lambda * (1 - dones_float[t]) * gae
        advantages.insert(0, gae)
        next_value = values[t]

    advantages = torch.tensor(advantages)
    returns = advantages + values
    # æ›´ç¨³å®šçš„ä¼˜åŠ¿æ ‡å‡†åŒ–
    adv_mean = advantages.mean()
    adv_std = advantages.std()
    if adv_std < 1e-8:
        advantages = (advantages - adv_mean)
    else:
        advantages = (advantages - adv_mean) / (adv_std + 1e-8)
    return advantages, returns

# è®­ç»ƒå‡½æ•°
def train_on_batch_ppo(batch, gamma=0.99, gae_lambda=0.95, clip_epsilon=0.2, entropy_coef=0.1, device="cpu", ep=0): #åŸç†µæ­£åˆ™ç³»æ•°0.01è¿‡ä½ï¼Œä¸åˆ©äºæ¢ç´¢  æé«˜è‡³0.1èƒ½æ›´å¥½å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨  é«˜ç†µç³»æ•°åœ¨è®­ç»ƒåˆæœŸå°¤ä¸ºé‡è¦

    states = torch.tensor(np.array([s["state"] for s in batch]), dtype=torch.float32).to(device)
    actions = torch.tensor(np.array([s["action_id"] for s in batch]), dtype=torch.long).to(device)
    rewards = torch.tensor(np.array([s["reward"] for s in batch]), dtype=torch.float32).to(device)
    old_log_probs = torch.tensor(np.array([s["log_prob"] for s in batch]), dtype=torch.float32).to(device)

    # è‡ªåŠ¨æ„é€  next_states å’Œ dones
    next_states = torch.zeros_like(states)
    next_states[:-1] = states[1:]
    next_states[-1] = 0.0
    dones = torch.zeros(len(batch), dtype=torch.bool, device=device)
    dones[-1] = True

    # === Critic ä¼°å€¼ ===
    values = critic(states).squeeze(-1)            # [batch]
    next_values = critic(next_states).squeeze(-1)  # [batch]
    next_values[dones] = 0.0

    # === è®¡ç®— GAE å’Œ Returns ===
    advantages, returns = compute_gae(rewards, values, dones, gamma, gae_lambda)

    # === PPO æ ¸å¿ƒï¼šClipped Surrogate Objective ===
    probs = actor(states)                          # [batch, action_dim]
    dist = Categorical(probs)
    new_log_probs = dist.log_prob(actions)
    entropy = dist.entropy().mean()                # ç†µæ­£åˆ™é¡¹

    # æ”¹è¿›çš„ç†µç³»æ•°è°ƒæ•´ï¼šå‰æœŸä¿æŒé«˜æ¢ç´¢ï¼ŒåæœŸç¼“æ…¢è¡°å‡
    if ep < 1000:
        entropy_coef = 0.08  # å‰æœŸä¿æŒè¾ƒé«˜æ¢ç´¢
    else:
        entropy_coef = max(0.03, 0.08 * (0.995 ** ((ep-1000)//100)))  # åæœŸç¼“æ…¢è¡°å‡
    
    # ç­–ç•¥æ¯”ç‡å’Œè£å‰ªæŸå¤±
    ratios = torch.exp(new_log_probs - old_log_probs)
    surr1 = ratios * advantages
    surr2 = torch.clamp(ratios, 1.0 - clip_epsilon, 1.0 + clip_epsilon) * advantages
    policy_loss = -torch.min(surr1, surr2).mean()

    # Critic æŸå¤±ï¼ˆå€¼å‡½æ•° clipping å¯é€‰ï¼‰
    # ä½¿ç”¨MSEæŸå¤±å¹¶æ·»åŠ ä»·å€¼å‡½æ•°æ­£åˆ™åŒ–
    value_clipped = values + (returns - values).clamp(-clip_epsilon, clip_epsilon)
    critic_loss1 = F.smooth_l1_loss(values, returns)
    critic_loss2 = F.smooth_l1_loss(values, value_clipped)
    critic_loss = torch.max(critic_loss1, critic_loss2).mean()
    
    # æ·»åŠ ä»·å€¼å‡½æ•°æ­£åˆ™åŒ–é¡¹ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰
    value_reg = 0.001 * (values ** 2).mean()
    critic_loss += value_reg

    # æ€»æŸå¤±
    total_loss = policy_loss + 0.5 * critic_loss - entropy_coef * entropy

    # æ›´æ–°å‚æ•°
    actor_optimizer.zero_grad()
    critic_optimizer.zero_grad()
    total_loss.backward()
    
    # æ¢¯åº¦è£å‰ªé˜²æ­¢çˆ†ç‚¸
    torch.nn.utils.clip_grad_norm_(actor.parameters(), max_norm=1.0)
    torch.nn.utils.clip_grad_norm_(critic.parameters(), max_norm=1.0)
    
    actor_optimizer.step()
    critic_optimizer.step()

    return policy_loss.item(), critic_loss.item(), entropy.item()

# æ¨¡æ‹Ÿè®­ç»ƒæµç¨‹
def run_training(episodes=3000):
    os.makedirs("models", exist_ok=True)
    for ep in range(initial_ep, initial_ep + episodes): # ä»ä¸Šæ¬¡çš„epç»§ç»­
        game = GuandanGame(verbose=False)
        memory = []
        game.log(f"\nğŸ® æ¸¸æˆå¼€å§‹ï¼å½“å‰çº§ç‰Œï¼š{RANKS[game.active_level - 2]}")

        while True:
            if game.is_game_over or len(game.history) > 200:  # å¦‚æœæ¸¸æˆç»“æŸï¼Œç«‹å³è·³å‡ºå¾ªç¯
                break
            player = game.players[game.current_player]
            active_players = 4 - len(game.ranking)

            # **å¦‚æœ Pass çš„äºº == "å½“å‰æœ‰æ‰‹ç‰Œçš„ç©å®¶æ•° - 1"ï¼Œå°±é‡ç½®è½®æ¬¡**
            if game.pass_count >= (active_players - 1) and game.current_player not in game.ranking:
                if game.jiefeng:
                    first_player = game.ranking[-1]
                    teammate = 2 if first_player == 0 else 0 if first_player == 2 else 3 if first_player == 1 else 1
                    game.log(f"\nğŸ†• è½®æ¬¡é‡ç½®ï¼ç©å®¶ {teammate + 1} æ¥é£ã€‚\n")
                    game.recent_actions[game.current_player] = []  # è®°å½•ç©ºåˆ—è¡¨
                    game.current_player = (game.current_player + 1) % 4
                    game.last_play = None  # âœ… å…è®¸æ–°çš„è‡ªç”±å‡ºç‰Œ
                    game.pass_count = 0  # âœ… Pass è®¡æ•°å½’é›¶
                    game.is_free_turn = True
                    game.jiefeng = False
                else:
                    game.log(f"\nğŸ†• è½®æ¬¡é‡ç½®ï¼ç©å®¶ {game.current_player + 1} å¯ä»¥è‡ªç”±å‡ºç‰Œã€‚\n")
                    game.last_play = None  # âœ… å…è®¸æ–°çš„è‡ªç”±å‡ºç‰Œ
                    game.pass_count = 0  # âœ… Pass è®¡æ•°å½’é›¶
                    game.is_free_turn = True

            if game.current_player == 0:
                # 1. æ¨¡å‹æ¨ç†ï¼ˆåœ¨æŒ‡å®šè®¾å¤‡ä¸Šï¼‰
                state = game._get_obs()
                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)
                mask = torch.tensor(game.get_valid_action_mask(player.hand, M, game.active_level, game.last_play), 
                                   dtype=torch.float32).unsqueeze(0).to(device)
                mask = mask.squeeze(0)
                
                # ä¸´æ—¶åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼å¤„ç†å•ä¸ªæ ·æœ¬
                actor.eval()
                with torch.no_grad():
                    probs = actor(state_tensor, mask)
                    dist = Categorical(probs)
                    action = dist.sample()
                    log_prob = dist.log_prob(action)
                    action_id = action.item()
                    
                actor.train()  # åˆ‡æ¢å›è®­ç»ƒæ¨¡å¼
                action_struct = M_id_dict[action_id]
                    
                # 2. æšä¸¾æ‰€æœ‰åˆæ³•å‡ºç‰Œç»„åˆï¼ˆå¸¦èŠ±è‰²ï¼‰
                combos = enumerate_colorful_actions(action_struct, player.hand, game.active_level)
                if combos:
                    chosen_move = random.choice(combos)
                    if not chosen_move:
                        game.log(f"ç©å®¶ {game.current_player + 1} Pass")
                        game.pass_count += 1
                        game.recent_actions[game.current_player] = ['Pass']  # è®°å½• Pass
                    else:
                        # å¦‚æœ chosen_move ä¸ä¸ºç©ºï¼Œç»§ç»­è¿›è¡Œæ­£å¸¸çš„å‡ºç‰Œé€»è¾‘
                        game.last_play = chosen_move
                        game.last_player = game.current_player
                        for card in chosen_move:
                            player.played_cards.append(card)
                            player.hand.remove(card)
                        game.log(f"ç©å®¶ {game.current_player + 1} å‡ºç‰Œ: {' '.join(chosen_move)}")
                        game.recent_actions[game.current_player] = list(chosen_move)  # è®°å½•å‡ºç‰Œ
                        game.jiefeng = False
                        if not player.hand:  # ç©å®¶å‡ºå®Œç‰Œ
                            game.log(f"\nğŸ‰ ç©å®¶ {game.current_player + 1} å‡ºå®Œæ‰€æœ‰ç‰Œï¼\n")
                            game.ranking.append(game.current_player)
                            if len(game.ranking) <= 2:
                                game.jiefeng = True

                        game.pass_count = 0
                        if not player.hand:
                            game.pass_count -= 1

                        if game.is_free_turn:
                            game.is_free_turn = False
                else:
                    game.log(f"ç©å®¶ {game.current_player + 1} Pass")
                    game.pass_count += 1
                    game.recent_actions[game.current_player] = ['Pass']  # è®°å½• Pass

                # å¼ºåŒ–å¥–åŠ±ä¿¡å·è®¾è®¡
                entry = find_entry_by_id(M, action_id)
                reward = 0
                
                # æ”¹è¿›çš„å¥–åŠ±å‡½æ•°è®¾è®¡
                reward = 0
                hand_size_before = len(player.hand)
                
                # åˆæ³•åŠ¨ä½œåŸºç¡€å¥–åŠ±
                if mask[action_id] and action_id != 0:  # éPassåŠ¨ä½œ
                    # åŸºç¡€å¥–åŠ± = ç‰Œå‹ç‚¹æ•° * é€»è¾‘ç‚¹å€’æ•°
                    base_reward = float(len(entry['points'])) * (1 / entry['logic_point'])
                    
                    # ç‚¸å¼¹ç±»ç‰Œå‹é¢å¤–å¥–åŠ±ï¼ˆä½¿ç”¨å¹³æ–¹æ ¹ç¼©æ”¾ï¼‰
                    if 120 <= action_id <= 364: 
                        bomb_strength = min(math.sqrt(entry['logic_point']), 2.0)  # å¹³æ–¹æ ¹ç¼©æ”¾
                        base_reward *= 1.0 + bomb_strength
                    
                    # æ‰‹ç‰Œå‡å°‘å¥–åŠ±ï¼ˆçº¿æ€§+é˜¶æ®µå¥–åŠ±ï¼‰
                    cards_played = hand_size_before - len(player.hand)
                    hand_reward = 0.03 * cards_played
                    
                    # é˜¶æ®µå¥–åŠ±ï¼šæ ¹æ®å‰©ä½™æ‰‹ç‰Œæ•°ç»™äºˆé¢å¤–å¥–åŠ±
                    if len(player.hand) <= 5:
                        hand_reward += 0.1 * (10 - len(player.hand))
                    
                    reward = base_reward + hand_reward
                    
                # éæ³•åŠ¨ä½œæƒ©ç½šï¼ˆåŸºäºé˜¶æ®µï¼‰
                elif not mask[action_id]:
                    # æƒ©ç½šéšæ¸¸æˆé˜¶æ®µå¢åŠ 
                    penalty_factor = 1.0 + (20 - hand_size_before) * 0.05
                    reward = -1.0 * penalty_factor
                
                memory.append({"state": state, "action_id": action_id, "reward": reward,"log_prob": log_prob.item()})
                player.last_played_cards = game.recent_actions[game.current_player]
                game.current_player = (game.current_player + 1) % 4
            else:
                game.ai_play(player)  # å…¶ä»–äººç”¨éšæœº
            # **è®°å½•æœ€è¿‘ 5 è½®å†å²**è®°å½•å†å²ï¼ˆæ¯è½®ç»“æŸæ—¶ï¼‰
            if game.current_player == 0 and any(action != ['None'] for action in game.recent_actions):
                round_history = [game.recent_actions[i].copy() for i in range(4)]
                game.history.append(round_history)
                
                # é‡ç½®æœ€è¿‘åŠ¨ä½œï¼ˆä½¿ç”¨æ·±æ‹·è´é¿å…å¼•ç”¨é—®é¢˜ï¼‰
                game.recent_actions = [['None'] for _ in range(4)]

        # å›¢é˜Ÿå¥–åŠ±ï¼šä½¿ç”¨åˆ†æ®µå‡½æ•°
        if game.upgrade_amount > 0:
            team_reward = min(5.0, game.upgrade_amount * 1.5)  # ä¸Šé™5.0
        else:
            team_reward = 0.0
        
        # æ ¹æ®ç©å®¶ä½ç½®åˆ†é…å¥–åŠ±ï¼ˆç©å®¶0å±äº0é˜Ÿï¼Œé˜Ÿå‹æ˜¯ç©å®¶2ï¼‰
        player_position = 0
        team_id = 0 if player_position in [0, 2] else 1
        team_multiplier = 1 if team_id == 0 else -1
        
        if memory:  # ç¡®ä¿ memory ä¸ä¸ºç©º
            # å°†å›¢é˜Ÿå¥–åŠ±åˆ†é…ç»™æ•´ä¸ªepisode
            for i in range(len(memory)):
                # æ”¹è¿›çš„å¥–åŠ±åˆ†é…ï¼šåŸºäºæ—¶é—´æ­¥çš„çº¿æ€§è¡°å‡
                decay = max(0.2, 1.0 - i / len(memory))  # çº¿æ€§è¡°å‡ï¼Œæœ€å°ä¿ç•™20%
                memory[i]["reward"] += team_multiplier * team_reward * decay
                
            al, cl, entropy = train_on_batch_ppo(memory, entropy_coef=0.1, device=device, ep=ep)
            
            # ä½¿ç”¨æŸå¤±æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
            actor_scheduler.step(al)
            critic_scheduler.step(cl)
            
            if (ep + 1) % 50 == 0:
                # è®¡ç®—å¥–åŠ±ç»Ÿè®¡
                rewards = [s["reward"] for s in memory]
                avg_reward = sum(rewards) / len(rewards) if rewards else 0
                max_reward = max(rewards) if rewards else 0
                min_reward = min(rewards) if rewards else 0
                
                # è®¡ç®—ç­–ç•¥ç†µï¼ˆä½¿ç”¨memoryä¸­çš„çŠ¶æ€ï¼‰
                if memory:
                    # ä»memoryä¸­æå–æ‰€æœ‰çŠ¶æ€
                    states = torch.tensor(np.array([s["state"] for s in memory]), dtype=torch.float32).to(device)
                    with torch.no_grad():
                        probs = actor(states)
                        dist = Categorical(probs)
                        policy_entropy = dist.entropy().mean().item()
                else:
                    policy_entropy = 0.0
                
                # è®¡ç®—åŠ¨ä½œåˆ†å¸ƒç†µ
                action_probs = probs.mean(dim=0)
                action_entropy = Categorical(action_probs).entropy().item()
                
                print(f"Episode {ep + 1}: "
                      f"ALoss={al:.4f}, CLoss={cl:.4f}, "
                      f"Entropy={entropy:.4f}, ActionEntropy={action_entropy:.4f}, "
                      f"Reward(avg={avg_reward:.2f}, max={max_reward:.2f}, min={min_reward:.2f}), "
                      f"LR(actor={actor_optimizer.param_groups[0]['lr']:.2e}, critic={critic_optimizer.param_groups[0]['lr']:.2e})")

        if (ep + 1) % 200 == 0:
            torch.save(actor.state_dict(), f"models/actor_ppo_ep{ep + 1}.pth")
            torch.save(critic.state_dict(), f"models/critic_ppo_ep{ep + 1}.pth")

if __name__ == "__main__":
    run_training()
