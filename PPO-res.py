"""
æ”¹è¿›çš„PPOç®—æ³•å®ç° - æ¼è›‹çº¸ç‰Œæ¸¸æˆAIè®­ç»ƒ
æ ¸å¿ƒç›®æ ‡ï¼šé™ä½AIé€‰æ‹©Passçš„æ¦‚ç‡ï¼Œé¼“åŠ±ç§¯æå‡ºç‰Œ
"""

import os
import sys
import time
import math
import threading
import psutil
import os
import numpy as np
from pathlib import Path
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical
import json
from guandan_env import GuandanGame
from get_actions import enumerate_colorful_actions
import random
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime
import logging
from copy import deepcopy

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler('training.log'),
        logging.StreamHandler()
    ]
)

# åŠ è½½åŠ¨ä½œé…ç½®
try:
    with open("guandan_actions.json", "r", encoding="utf-8") as f:
        M = json.load(f)
    action_dim = len(M)
    M_id_dict = {a['id']: a for a in M}
except FileNotFoundError:
    logging.error("æœªæ‰¾åˆ°guandan_actions.jsonæ–‡ä»¶")
    raise

RANKS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', 'J', 'Q', 'K', 'A']
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logging.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

class ReplayBuffer:
    """å­˜å‚¨è®­ç»ƒæ ·æœ¬çš„å¾ªç¯ç¼“å†²åŒº"""
    def __init__(self, capacity=20000):
        self.buffer = []  # å­˜å‚¨æ ·æœ¬çš„åˆ—è¡¨
        self.capacity = capacity # æœ€å¤§å®¹é‡
        self.position = 0
        self.lock = threading.Lock()  # æ·»åŠ çº¿ç¨‹é”
        
    def push(self, state, action, reward, next_state, done, log_prob):
        """æ·»åŠ æ–°æ ·æœ¬"""
        # ç¡®ä¿æ•°æ®æœ‰æ•ˆ
        if state is None or next_state is None or log_prob is None:
            return
        
        # ä½¿ç”¨çº¿ç¨‹é”ç¡®ä¿çº¿ç¨‹å®‰å…¨
        with self.lock:
            # å½“ç¼“å†²åŒºæœªæ»¡æ—¶ï¼Œç›´æ¥æ·»åŠ æ–°å…ƒç´ 
            if len(self.buffer) < self.capacity:
                self.buffer.append((state, action, reward, next_state, done, log_prob))
            else:
                # ç¼“å†²åŒºå·²æ»¡æ—¶ï¼Œè¦†ç›–æœ€æ—§çš„æ•°æ®
                self.buffer[self.position] = (state, action, reward, next_state, done, log_prob)
            self.position = (self.position + 1) % self.capacity
        
    def sample(self, batch_size):
        """éšæœºé‡‡æ ·ä¸€æ‰¹æ ·æœ¬"""
        # è¿‡æ»¤æ‰Noneå€¼
        valid_buffer = [item for item in self.buffer if item is not None]
        if not valid_buffer:
            # è¿”å›ç©ºæ•°ç»„è€Œä¸æ˜¯Noneï¼Œé¿å…è§£åŒ…é”™è¯¯
            return np.array([]), np.array([]), np.array([]), np.array([]), np.array([]), np.array([])
        
        # ä»æœ‰æ•ˆç¼“å†²åŒºé‡‡æ ·ï¼ˆé¿å…é‡‡æ ·åˆ°Noneï¼‰
        batch = random.sample(valid_buffer, min(batch_size, len(valid_buffer)))
        states, actions, rewards, next_states, dones, log_probs = map(np.stack, zip(*batch))
        return states, actions, rewards, next_states, dones, log_probs
        
    def __len__(self):
        return len(self.buffer)

# ä¿®æ”¹SharedBackboneç±»
class SharedBackbone(nn.Module):
    def __init__(self, state_dim=3049, hidden_dim=1024):
        super().__init__()
        
        self.input_bn = nn.BatchNorm1d(state_dim)
        self.hidden_dim=hidden_dim
        self.eval_mode = False
        
        # æ‰‹ç‰Œç¼–ç å™¨
        self.card_encoder = nn.Sequential(
            nn.Linear(108, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(0.2)
        )
        
        # å†å²åŠ¨ä½œç¼–ç å™¨
        self.history_encoder = nn.Sequential(
            nn.Linear(2160, 512),
            nn.LayerNorm(512),
            nn.GELU(),
            nn.Dropout(0.2)
        )
        
        # åœºæ™¯ä¿¡æ¯ç¼–ç å™¨
        self.context_encoder = nn.Sequential(
            nn.Linear(781, 256),
            nn.LayerNorm(256),
            nn.GELU(),
            nn.Dropout(0.2)
        )
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=8,  # å‡å°‘æ³¨æ„åŠ›å¤´æ•°
            dim_feedforward=1024,  # å‡å°å‰é¦ˆå±‚ç»´åº¦
            dropout=0.1,
            activation='gelu',
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=6  # å‡å°‘Transformerå±‚æ•°
        )
        
        # ä½ç½®ç¼–ç 
        self.position_embedding = nn.Parameter(
            torch.randn(1, 3, hidden_dim) * 0.02
        )
        
        # åˆ†ç±»token
        self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_dim))
        
        # è¾“å‡ºæŠ•å½±å±‚
        self.proj = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Dropout(0.1)
        )

    def forward(self, x):
        x = self.input_bn(x)
        
        # æå–å„ç‰¹å¾
        card_feat = self.card_encoder(x[..., :108])  # [B, 512]
        history_feat = self.history_encoder(x[..., 108:2268])  # [B, 512]
        context_feat = self.context_encoder(x[..., 2268:])  # [B, 256]
        
        # æŠ•å½±åˆ°ç»Ÿä¸€ç»´åº¦
        card_feat = F.linear(card_feat, torch.zeros(self.hidden_dim, 512).to(x.device))
        history_feat = F.linear(history_feat, torch.zeros(self.hidden_dim, 512).to(x.device))
        context_feat = F.linear(context_feat, torch.zeros(self.hidden_dim, 256).to(x.device))
        
        # æ·»åŠ åˆ†ç±»tokenå’Œä½ç½®ç¼–ç 
        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)  # [B, 1, D]
        features = torch.stack([card_feat, history_feat, context_feat], dim=1)  # [B, 3, D]
        features = features + self.position_embedding
        features = torch.cat([cls_tokens, features], dim=1)  # [B, 4, D]
        
        # Transformerå¤„ç†
        features = self.transformer_encoder(features)
        
        # å–åˆ†ç±»tokenä½œä¸ºè¾“å‡º
        out = self.proj(features[:, 0])
        
        return out
    
    def eval(self):
        """è®¾ç½®è¯„ä¼°æ¨¡å¼"""
        super().eval()
        self.eval_mode = True
        
    def train(self, mode=True):
        """è®¾ç½®è®­ç»ƒæ¨¡å¼"""
        super().train(mode)
        self.eval_mode = not mode
            
class ImprovedResNetActor(nn.Module):
    """æ”¹è¿›çš„ç­–ç•¥ç½‘ç»œ"""
    def __init__(self, backbone, action_dim=action_dim):
        super().__init__()
        self.backbone = backbone
        backbone_out_dim = backbone.hidden_dim  # ä½¿ç”¨Transformerçš„éšè—ç»´åº¦
        
        # æ›´æ·±çš„ç­–ç•¥å¤´
        self.fc_policy = nn.Sequential(
            nn.Linear(backbone_out_dim, backbone_out_dim//2),
            nn.LayerNorm(backbone_out_dim//2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(backbone_out_dim//2, backbone_out_dim//4),
            nn.LayerNorm(backbone_out_dim//4),
            nn.GELU(),
            nn.Linear(backbone_out_dim//4, action_dim),
            nn.LayerNorm(action_dim)
        )
        
    def forward(self, x, mask=None):
        features = self.backbone(x)
        logits = self.fc_policy(features)
        
        # åŠ¨æ€Passæƒ©ç½šç³»æ•°
        pass_penalty = 1.5 if self.training else 1.0  # è®­ç»ƒæ—¶æ›´é«˜æƒ©ç½š
        # ä»çŠ¶æ€å¼ é‡ä¸­æå–æ‰‹ç‰Œæ•°é‡ï¼ˆå‰108ç»´æ˜¯æ‰‹ç‰Œç‰¹å¾ï¼‰
        hand_size = int(x[..., :108].sum().item())  # æ‰‹ç‰Œæ•°é‡ = éé›¶ç‰¹å¾æ•°é‡
        if hand_size < 5:  # ç»ˆå±€é˜¶æ®µé™ä½Passæƒ©ç½š
            pass_penalty *= max(0.5, 1 - (5 - hand_size)*0.1)
        logits[..., 0] -= pass_penalty
        
        # åº”ç”¨åŠ¨ä½œæ©ç 
        if mask is not None:
            logits = logits + (mask.float() - 1) * 1e9
        
        probs = F.softmax(logits, dim=-1)
        return probs, logits

class ImprovedResNetCritic(nn.Module):
    """ä»·å€¼ç½‘ç»œ"""
    def __init__(self, backbone):
        super().__init__()
        self.backbone = backbone
        backbone_out_dim = backbone.hidden_dim  # ä½¿ç”¨Transformerçš„éšè—ç»´åº¦
        self.fc_value = nn.Sequential(
            nn.Linear(backbone_out_dim, backbone_out_dim//2),
            nn.LayerNorm(backbone_out_dim//2),
            nn.ReLU(),
            nn.Dropout(0.1),  # Add dropout
            nn.Linear(backbone_out_dim//2, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Linear(128, 1),
            nn.Tanh() # è¾“å‡ºåœ¨[-1,1]èŒƒå›´
        )
        self._init_weights()
    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.orthogonal_(m.weight, gain=1.0)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0.0)
    def forward(self, x):
        features = self.backbone(x)
        value = self.fc_value(features)
        return value

# é¢„åˆ†é…GPUå†…å­˜
state_buf = torch.empty((8192, 3049), dtype=torch.float32, device=device)
mask_buf = torch.empty((8192, 456), dtype=torch.float32, device=device)

def bind_to_core(core_id):
    p = psutil.Process(os.getpid())
    p.cpu_affinity([core_id])
    
def validate_game_state(game):
    """éªŒè¯æ¸¸æˆçŠ¶æ€çš„åˆæ³•æ€§"""
    # æ£€æŸ¥å‡ºç‰Œæƒ
    if game.is_free_turn and game.last_play:
        logging.warning("çŠ¶æ€é”™è¯¯ï¼šè‡ªç”±å‡ºç‰Œå›åˆå­˜åœ¨last_play")
        game.last_play = []
        
    # æ£€æŸ¥æ’å
    if len(set(game.ranking)) != len(game.ranking):
        logging.error("çŠ¶æ€é”™è¯¯ï¼šæ’åé‡å¤")
        game.ranking = list(dict.fromkeys(game.ranking))
        
    # æ£€æŸ¥Passè®¡æ•°
    if game.pass_count > 4:
        logging.warning("çŠ¶æ€é”™è¯¯ï¼špass_countè¶…è¿‡4")
        game.pass_count = 0
        
    return True

def calculate_team_reward(game):
    """è®¡ç®—é˜Ÿä¼è·èƒœå¥–åŠ±
    è¾“å…¥: gameå¯¹è±¡
    è¾“å‡º: å¥–åŠ±å€¼(é’ˆå¯¹ç©å®¶0)
    """
    if not game.is_game_over or len(game.ranking) < 2:
        return 0.0
    
    # è·å–å‰ä¸¤åç©å®¶
    top_two = game.ranking[:2]
    
    # åˆ¤æ–­é˜Ÿä¼0è·èƒœæ¡ä»¶ï¼šç©å®¶0å’Œ2éƒ½åœ¨å‰ä¸¤å
    team0_win = (0 in top_two) and (2 in top_two)
    
    # åˆ¤æ–­é˜Ÿä¼1è·èƒœæ¡ä»¶ï¼šç©å®¶1å’Œ3éƒ½åœ¨å‰ä¸¤å
    team1_win = (1 in top_two) and (3 in top_two)
    
    # é˜Ÿä¼è·èƒœå¥–åŠ±
    if team0_win:
        return 2.0  # é˜Ÿä¼0è·èƒœå¥–åŠ±
    elif team1_win:
        return -2.0  # é˜Ÿä¼1è·èƒœæƒ©ç½š
    else:
        # æ··åˆæ’åæƒ…å†µ
        if 0 in top_two:
            return 1.5  # ç©å®¶0è¿›å…¥å‰ä¸¤åå¥–åŠ±
        elif 2 in top_two:
            return 1.0  # é˜Ÿå‹ç©å®¶2è¿›å…¥å‰ä¸¤åå¥–åŠ±
        # æ–°å¢ï¼šåŒæ–¹é˜Ÿå‹éƒ½åœ¨å‰ä¸¤åä½†é¡ºåºä¸åŒ
        elif (0 in top_two and 3 in top_two) or (1 in top_two and 2 in top_two):
            return 0.5  # æ··åˆé˜Ÿä¼å¥–åŠ±
    return 0.0

def calculate_improved_reward(entry, player, mask, action_id, hand_size_before, game, ep):
    """æ”¹è¿›çš„å¥–åŠ±è®¡ç®—"""
    reward = 0.0
    progress = (27 - len(player.hand)) / 27  # æ¸¸æˆè¿›åº¦[0,1]
    
    # åŠ¨æ€åŸºç¡€å¥–åŠ± - éšæ¸¸æˆè¿›åº¦å¢åŠ 
    base_reward = 0.1 + progress * 0.2
    reward += base_reward
    
    # ç‚¸å¼¹ä½¿ç”¨å¥–åŠ±è°ƒæ•´
    bomb_bonus = {
        'bomb': 0.4 + progress*0.3,
        'straight_bomb': 0.7 + progress*0.4,
        'joker_bomb': 1.2  # é™ä½å¤©ç‹ç‚¸å¥–åŠ±
    }
    hand_size = len(player.hand)
    progress = (27 - hand_size) / 27
    
    # æ˜¥å¤©åˆ¤æ–­ - ç¬¬ä¸€è½®å°±å‡ºå®Œæ‰€æœ‰ç‰Œ
    if game.current_round == 1 and hand_size == 0:
        reward += 2.5  # æ˜¥å¤©é¢å¤–å¥–åŠ±
        game.log(f"ğŸ‰ æ˜¥å¤©ï¼ç©å®¶ {game.current_player + 1} åœ¨ç¬¬ä¸€è½®å°±å‡ºå®Œæ‰€æœ‰ç‰Œï¼")
    
    # Passå¤„ç†ä¼˜åŒ–
        if action_id == 0:
            # åŠ¨æ€Passæƒ©ç½š
            base_penalty = 0.5  # åŸºç¡€æƒ©ç½šå€¼
            # æ ¹æ®æ¸¸æˆé˜¶æ®µè°ƒæ•´
            if progress < 0.3:  # æ—©æœŸé˜¶æ®µ
                penalty = -base_penalty * 0.8
            elif progress > 0.7:  # ç»ˆå±€é˜¶æ®µ
                penalty = -base_penalty * 1.5
            else:
                penalty = -base_penalty
                
            # è‡ªç”±å‡ºç‰Œè½®æ¬¡Passæƒ©ç½šåŠ å€
            if game.is_free_turn:
                penalty *= 2.0
                
            reward += penalty
            
            # è¿ç»­Passé¢å¤–æƒ©ç½š
            if game.pass_count > 2:
                reward -= 0.1 * game.pass_count
        
    else:
        # å‡ºç‰Œå¥–åŠ±ä¼˜åŒ–
        action_type = entry.get('type', '')
        cards_played = hand_size_before - hand_size
        
        # åŸºç¡€å¥–åŠ±(è€ƒè™‘å‰©ä½™ç‰Œæ•°)
        base_reward = 0.4 + progress * 0.3
        if hand_size <= len(player.hand) / 2:
            base_reward *= 1.2  # ç‰Œæ•°å°‘äºä¸€åŠæ—¶å¢åŠ å¥–åŠ±
            
        # ç‰Œå‹å¥–åŠ±ä¼˜åŒ–
        bomb_multiplier = 1.0
        # å…³é”®å›åˆç‚¸å¼¹å¥–åŠ±åŠ æˆï¼ˆç»ˆå±€æˆ–å¯¹æŠ—å¤§ç‰Œï¼‰
        if progress > 0.7 and len(game.last_play) >= 4:
            bomb_multiplier = 1.5
            
        type_bonus = {
            'single': 0.2 + (1-progress)*0.2,
            'pair': 0.25 + progress*0.1,
            'trio': 0.3 + progress*0.15,
            'bomb': (0.4 + progress*0.2) * bomb_multiplier,  # æ™®é€šç‚¸å¼¹
            'straight_bomb': (0.6 + progress*0.3) * bomb_multiplier,  # é¡ºå­ç‚¸å¼¹
            'joker_bomb': 1.0 * bomb_multiplier,  # å¤©ç‹ç‚¸
            'sequence': 0.35 + len(game.last_play)*0.04,
            'spring': 5.0  # æ˜¥å¤©å¥–åŠ±
        }.get(action_type, 0.0)
        
        # ç‚¸å¼¹ä½¿ç”¨æˆæœ¬ï¼ˆéšæ¸¸æˆè¿›åº¦å‡å°‘ï¼‰
        if 'bomb' in action_type:
            bomb_cost = max(0.1, 0.3 * (1 - progress))
            type_bonus -= bomb_cost
        
        # æ§åˆ¶æƒå¥–åŠ±
        control_bonus = 0.0
        if game.last_player == 0:
            control_bonus = 0.2 + progress * 0.15
            
        # å…³é”®ç‰Œå¥–åŠ±
        key_card_bonus = 0.0
        if any(c[0] in ['A', '2'] for c in game.recent_actions[0]):
            key_card_bonus = 0.15 + progress * 0.1
            
        # å¢å¼ºé˜Ÿå‹é…åˆå¥–åŠ±
        teammate_bonus = 0.0
        teammate = (game.current_player + 2) % 4
        
        # åŸºç¡€é˜Ÿå‹é…åˆ
        if game.last_player == teammate:
            teammate_bonus = 0.2
            
        # æ¥é˜Ÿå‹ç‰Œå‹é¢å¤–å¥–åŠ±
        if (game.last_player == teammate and 
            action_type == game.last_play_type and
            cards_played >= len(game.last_play)):
            teammate_bonus += 0.15
            
        # ä¸ºé˜Ÿå‹åˆ›é€ æœºä¼šå¥–åŠ±
        if (game.last_player != teammate and 
            len(player.hand) < 10 and 
            cards_played == 1 and 
            '2' in game.recent_actions[0][0]):
            teammate_bonus += 0.1
            
        # ç­–ç•¥æ€§é¢å¤–å¥–åŠ±
        strategy_bonus = 0.0
        # å‹åˆ¶å¯¹æ‰‹å¥–åŠ±
        if game.last_player in [1, 3] and action_type == game.last_play_type:
            strategy_bonus += 0.3
        # é…åˆé˜Ÿå‹å¥–åŠ±
        if game.last_player in [0, 2] and action_type == game.last_play_type:
            strategy_bonus += 0.2
        # è·å¾—å‡ºç‰Œæƒå¥–åŠ±
        if game.is_free_turn:
            strategy_bonus += 0.1
            
        reward += (base_reward + type_bonus + control_bonus + 
                  key_card_bonus + teammate_bonus + strategy_bonus)
        
        # ç»ˆå±€ä¼˜åŒ–
        if hand_size <= 3:
            reward += (4 - hand_size) * 0.4
            if hand_size == 0:  # å‡ºå®Œç‰Œé¢å¤–å¥–åŠ±
                reward += 2.0
                
    return reward


def compute_gae(rewards, values, next_values, dones, gamma=0.99, gae_lambda=0.95):
    batch_size = len(rewards)
    advantages = torch.zeros_like(rewards)
    gae = 0
    for t in reversed(range(batch_size)):
        if t == batch_size - 1:
            next_value = next_values[t]
        else:
            next_value = values[t + 1]
        delta = rewards[t] + gamma * next_value * (1 - dones[t].float()) - values[t]
        gae = delta + gamma * gae_lambda * (1 - dones[t].float()) * gae
        advantages[t] = gae
    returns = advantages + values
    # æ·»åŠ ä¼˜åŠ¿å€¼è£å‰ªå’Œæ›´ç¨³å®šçš„æ ‡å‡†åŒ–
    advantages = torch.clamp(advantages, -3.0, 3.0)
    adv_mean = advantages.mean()
    adv_std = advantages.std(unbiased=False) + 1e-7
    advantages = (advantages - adv_mean) / adv_std
    return advantages, returns

def save_checkpoint(backbone, actor, critic, optimizer, ep, model_dir="models"):
    os.makedirs(model_dir, exist_ok=True)
    checkpoint = {
        'backbone_state_dict': backbone.state_dict(),
        'actor_state_dict': actor.state_dict(),
        'critic_state_dict': critic.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'episode': ep
    }
    torch.save(checkpoint, f"{model_dir}/checkpoint_ep{ep}.pth")
    logging.info(f"ä¿å­˜æ£€æŸ¥ç‚¹: checkpoint_ep{ep}.pth")

def load_checkpoint(device, backbone, actor, critic, optimizer, model_dir="models"):
    model_files = sorted(Path(model_dir).glob("checkpoint_ep*.pth"))
    if not model_files:
        logging.info("æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
        return 0
    latest_checkpoint = str(model_files[-1])
    checkpoint = torch.load(latest_checkpoint, map_location=device)
    backbone.load_state_dict(checkpoint['backbone_state_dict'])
    actor.load_state_dict(checkpoint['actor_state_dict'])
    critic.load_state_dict(checkpoint['critic_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    ep = checkpoint['episode']
    logging.info(f"åŠ è½½æ£€æŸ¥ç‚¹: {latest_checkpoint}")
    return ep


def train_on_batch_ppo(states, actions, rewards, next_states, dones, old_log_probs,
                      backbone, actor, critic, target_critic, optimizer,
                      gamma=0.995, gae_lambda=0.95, device=device, ep=0):
    """ä¼˜åŒ–åçš„PPOè®­ç»ƒå‡½æ•°"""
    states = torch.FloatTensor(states).to(device)
    next_states = torch.FloatTensor(next_states).to(device)
    actions = torch.LongTensor(actions).to(device)
    rewards = torch.FloatTensor(rewards).to(device)
    dones = torch.BoolTensor(dones).to(device)
    old_log_probs = torch.FloatTensor(old_log_probs).to(device)

    # æ ‡å‡†åŒ–rewards
    rewards = torch.clamp(rewards, -5.0, 5.0)  # å…ˆè£å‰ªæç«¯å€¼
    rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)

    # è®¡ç®—ä¼˜åŠ¿å€¼å’Œå›æŠ¥
    with torch.no_grad():
        next_values = target_critic(next_states).squeeze(-1)
    values = critic(states).squeeze(-1)
    
    # ä½¿ç”¨TD(Î»)è®¡ç®—ä¼˜åŠ¿
    advantages = torch.zeros_like(rewards)
    returns = torch.zeros_like(rewards)
    gae = 0
    for t in reversed(range(len(rewards))):
        if t == len(rewards) - 1:
            next_value = next_values[t]
        else:
            next_value = values[t + 1]
        delta = rewards[t] + gamma * next_value * (1 - dones[t].float()) - values[t]
        gae = delta + gamma * gae_lambda * (1 - dones[t].float()) * gae
        advantages[t] = gae
        returns[t] = advantages[t] + values[t]
    
    # æ ‡å‡†åŒ–ä¼˜åŠ¿å€¼
    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

    # è®¡ç®—æ–°çš„åŠ¨ä½œæ¦‚ç‡
    probs, logits = actor(states)
    dist = Categorical(probs)
    new_log_probs = dist.log_prob(actions)
    entropy = dist.entropy().mean()
    
    # è®¡ç®—KLæ•£åº¦å¹¶åŠ¨æ€è°ƒæ•´clipèŒƒå›´
    kl_div = (old_log_probs - new_log_probs).mean()
    # åŠ¨æ€è°ƒæ•´clipèŒƒå›´ç¨³å®šè®­ç»ƒ
    if kl_div > 0.015:
        clip_epsilon = 0.1  # æ”¶ç´§clipèŒƒå›´
    elif kl_div < 0.005:
        clip_epsilon = 0.25  # æ”¾å®½clipèŒƒå›´
    else:
        clip_epsilon = 0.15  # ä¸­ç­‰èŒƒå›´
    
    # è®¡ç®—ç­–ç•¥æŸå¤±
    ratios = torch.exp(new_log_probs - old_log_probs)
    surr1 = ratios * advantages
    surr2 = torch.clamp(ratios, 1.0 - clip_epsilon, 1.0 + clip_epsilon) * advantages
    policy_loss = -torch.min(surr1, surr2).mean()
    
    # å¤§å¹…å¢å¼ºPassæƒ©ç½š
    pass_probs = probs[:, 0]
    pass_penalty_factor = 0.8  # å›ºå®šé«˜æƒ©ç½šå› å­
    pass_penalty = pass_penalty_factor * pass_probs.mean()
    policy_loss += pass_penalty  # ç›´æ¥åº”ç”¨æƒ©ç½š
    
    # Value Lossè®¡ç®—ä¼˜åŒ–
    value_pred = critic(states)
    value_targets = returns.unsqueeze(-1)
    value_loss = F.smooth_l1_loss(value_pred, value_targets)
    value_loss = value_loss.clamp(-8.0, 8.0)  # æ‰©å¤§value lossèŒƒå›´
    
    # åŠ¨æ€ç†µç³»æ•°
    if ep < 200:  # å»¶é•¿åˆå§‹æ¢ç´¢æœŸ
        entropy_coef = 0.03  # æé«˜åˆå§‹ç†µç³»æ•°
    else:
        entropy_coef = max(0.005, 0.03 * (0.9995 ** ((ep-200)//50)))  # é™ä½ç†µè¡°å‡é€Ÿåº¦
    
    # åŠ¨æ€è°ƒæ•´value lossæƒé‡
    value_loss_weight = min(1.0, 0.5 + ep/1000)  # éšè®­ç»ƒè¿›ç¨‹å¢åŠ value lossæƒé‡
    
    # æ€»æŸå¤±
    total_loss = policy_loss + value_loss_weight * value_loss - entropy_coef * entropy
    
    # æ£€æŸ¥æŸå¤±å€¼
    if torch.isnan(total_loss) or torch.isinf(total_loss):
        logging.warning(f"Invalid loss detected: {total_loss}")
        return policy_loss.item(), value_loss.item(), entropy.item(), kl_div.item()
    
    # ä¼˜åŒ–å™¨æ­¥éª¤
    optimizer.zero_grad()
    total_loss.backward()
    
    # æ¢¯åº¦è£å‰ªå’Œç¼©æ”¾
    max_grad_norm = 1.0
    grad_norm = torch.nn.utils.clip_grad_norm_(backbone.parameters(), max_grad_norm)
    if grad_norm > max_grad_norm:
        for param in backbone.parameters():
            if param.grad is not None:
                param.grad.data.mul_(max_grad_norm / (grad_norm + 1e-6))
    
    torch.nn.utils.clip_grad_norm_(actor.parameters(), max_grad_norm)
    torch.nn.utils.clip_grad_norm_(critic.parameters(), max_grad_norm)
    
    optimizer.step()
    
    return policy_loss.item(), value_loss.item(), entropy.item(), kl_div.item()

def run_training(episodes=30000):
    """æ”¹è¿›çš„è®­ç»ƒæµç¨‹ - æ·»åŠ è¯¾ç¨‹å­¦ä¹ å’Œç›®æ ‡ç½‘ç»œ"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    adaptive_params = {
        'min_batch_size': 1024,  # å‡å°æœ€å°batch size
        'max_batch_size': 8192,  # å‡å°æœ€å¤§batch size
        'batch_growth_interval': 256,  # å‡å°‘å¢åŠ é¢‘ç‡
        'current_batch_size': 1024,
        'growth_step': 256  # å‡å°å¢é•¿æ­¥é•¿
    }
    
    # æ”¹è¿›çš„è¯¾ç¨‹å­¦ä¹ 
    def get_curriculum(ep, win_rate):
        if ep < 1000:  # å»¶é•¿åˆå§‹è®­ç»ƒé˜¶æ®µ
            return {'level': (2,4), 'opponent': 'random'}
        elif ep < 3000:
            if win_rate < 0.45:
                return {'level': (3,6), 'opponent': 'random'}
            else:
                return {'level': (4,7), 'opponent': 'rule_based'}
        elif ep < 6000:
            if win_rate < 0.55:
                return {'level': (5,8), 'opponent': 'rule_based'}
            else:
                return {'level': (6,10), 'opponent': 'self'}
        else:
            return {'level': (8,14), 'opponent': 'self'}
    
    # å¯ç”¨cuDNN benchmarkæ¨¡å¼
    torch.backends.cudnn.benchmark = True
    
    # åˆå§‹åŒ–ç½‘ç»œ
    backbone = SharedBackbone().to(device)
    actor = ImprovedResNetActor(backbone).to(device)
    critic = ImprovedResNetCritic(backbone).to(device)
    
    # åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œï¼ˆç”¨äºç¨³å®šè®­ç»ƒï¼‰
    target_backbone = SharedBackbone().to(device)
    target_critic = ImprovedResNetCritic(target_backbone).to(device)
    
    # åŒæ­¥åˆå§‹å‚æ•°
    target_backbone.load_state_dict(backbone.state_dict())
    target_critic.load_state_dict(critic.state_dict())
    
    # ä¼˜åŒ–å™¨åˆ†ç»„è®¾ç½®
    optimizer_params = []
    
    # ä¼˜åŒ–å™¨è®¾ç½®
    optimizer_params = [
        {'params': [p for n,p in actor.named_parameters() 
                if not n.startswith('backbone.')],
         'lr': 3e-5,
         'weight_decay': 1e-5},
        {'params': [p for n,p in critic.named_parameters()
                if not n.startswith('backbone.')],
        'lr': 1.5e-4},  # é™ä½criticå­¦ä¹ ç‡
        {'params': backbone.parameters(),
        'lr': 5e-5}   # é™ä½backboneå­¦ä¹ ç‡
    ]

    optimizer = optim.AdamW(optimizer_params, weight_decay=1e-5)

    # å­¦ä¹ ç‡è°ƒåº¦å™¨ä¼˜åŒ–
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, 
        mode='min',
        patience=150,    # å¢åŠ patience
        factor=0.95,     # é™ä½å­¦ä¹ ç‡è¡°å‡ç³»æ•°
        min_lr=5e-6,
        verbose=True
    )
    
    '''
    # ä½¿ç”¨æ­¥æ•°è¡°å‡æ›¿ä»£Plateau
    scheduler = optim.lr_scheduler.StepLR(
        optimizer, 
        step_size=500,  # æ¯500æ­¥è¡°å‡ä¸€æ¬¡
        gamma=0.95      # è¡°å‡ç³»æ•°
    )
    '''
    
    num_collectors = 10  # æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´
    # åˆ›å»ºçº¿ç¨‹å®‰å…¨çš„dequeç¼“å†²åŒºåˆ—è¡¨
    memory_list = []
    for _ in range(num_collectors):
        memory_list.append(ReplayBuffer(capacity=20000))
        
    #memory = ReplayBuffer(capacity=50000)
    writer = SummaryWriter(f'runs/guandan_ppo_{datetime.now().strftime("%Y%m%d_%H%M%S")}')
    initial_ep = load_checkpoint(device, backbone, actor, critic, optimizer)
    best_reward = float('-inf')
    
    # åˆå§‹åŒ–è®­ç»ƒæŒ‡æ ‡
    policy_loss = float('inf')  # åˆå§‹åŒ–ä¸ºä¸€ä¸ªå¤§å€¼
    value_loss = 0
    entropy = 0
    kl_div = 0
    game_counter = 0  # ç‰Œå±€è®¡æ•°å™¨
    
    consecutive_pass_rounds = 0
        
    def soft_update(target, source, tau=0.001):
        for target_param, param in zip(target.parameters(), source.parameters()):
            target_param.data.copy_(
                target_param.data * (1.0 - tau) + param.data * tau
            )
    try:
        # åˆ›å»ºæ•°æ®æ”¶é›†å’Œè®­ç»ƒåˆ†ç¦»çš„çº¿ç¨‹
        from threading import Thread
        
        def data_collection_thread(id):
            
            p = psutil.Process()
            cores = list(range(psutil.cpu_count()))
            core_id = id % len(cores)
            p.cpu_affinity([core_id])
            print(f"çº¿ç¨‹ {id} ç»‘å®šåˆ°æ ¸å¿ƒ {core_id}")
            
            """ç‹¬ç«‹çš„æ•°æ®æ”¶é›†çº¿ç¨‹"""
            memory = memory_list[id]
            
            for ep in range(initial_ep, initial_ep + episodes):
                thread_id = threading.current_thread().ident
                game_counter = ep - initial_ep + 1
                run_id = datetime.now().strftime("%Y%m%d%H%M%S")
                game_id = f"{run_id}_{game_counter:04d}_{thread_id}"
                
                game = GuandanGame(verbose=False)  # å…³é—­è¯¦ç»†æ—¥å¿—
                game.game_id = game_id
                episode_reward = 0
                episode_steps = 0
                pass_penalty_given = False
                continue_rounds = 0                
                            
                while not game.is_game_over and len(game.history) <= 300:
                    # éªŒè¯æ¸¸æˆçŠ¶æ€
                    if not validate_game_state(game):
                        game.log("âš ï¸ æ¸¸æˆçŠ¶æ€éªŒè¯å¤±è´¥ï¼Œé‡ç½®çŠ¶æ€")
                        game.pass_count = 0
                        game.last_play = []
                        game.is_free_turn = True
                    
                    # è·³è¿‡å·²ç»å‡ºå®Œç‰Œçš„ç©å®¶ï¼ˆå³å·²ç»åœ¨æ’åä¸­çš„ç©å®¶ï¼‰    
                    while not game.is_game_over and game.current_player in game.ranking:
                        game.current_player = (game.current_player + 1) % 4
                        game.log(f"ç©å®¶ {game.current_player +1}  ERROR")
                        game.is_game_over = len(game.ranking) >= 4
                        game.check_game_over()
                        if game.is_game_over:
                            game.log(f"âš ï¸ æ‰€æœ‰ç©å®¶éƒ½å·²å‡ºå®Œç‰Œï¼Œå¼ºåˆ¶ç»“æŸæ¸¸æˆ ")

                    continue_rounds = (continue_rounds+1)%2
                    if continue_rounds == 0:
                        consecutive_pass_rounds = consecutive_pass_rounds+game.pass_count
                    else:
                        consecutive_pass_rounds=game.pass_count
                        
                    # æ£€æŸ¥è¿ç»­4æ¬¡Passæƒ©ç½š
                    if consecutive_pass_rounds > 8 and not pass_penalty_given:
                        # å¯¹æ‰€æœ‰ç©å®¶ï¼ˆè¿™é‡Œåªè®­ç»ƒä¸»æ™ºèƒ½ä½“ï¼Œrewardè®°å½•åœ¨memoryï¼‰
                        penalty = -1.0
                        # ä»…å¯¹ä¸»æ™ºèƒ½ä½“åšè®°å½•
                        if len(memory) > 0:
                            memory.push(memory.buffer[-1][0], 0, penalty, memory.buffer[-1][0], False, 0.0)
                            episode_reward += penalty
                        # æ—¥å¿—æç¤º
                        game.log(f"æ‰€æœ‰ç©å®¶è¿ç»­8æ¬¡Passï¼Œç»™äºˆæ‰€æœ‰ç©å®¶æƒ©ç½šï¼")
                        # æ¸…ç©ºpass_countå’Œrecent_actionsï¼Œè¿›å…¥æ–°ä¸€è½®
                        game.pass_count = 0
                        game.recent_actions = [['None'] for _ in range(4)]
                        pass_penalty_given = True
                    else:
                        pass_penalty_given = False
                        
                    # å½“ç©å®¶Aå‡ºç‰Œå®Œåï¼Œå…¶ä½™ç©å®¶éƒ½é€‰æ‹©Passï¼Œç©å®¶Aé‡æ–°è·å¾—è‡ªç”±å‡ºç‰Œæƒ
                    if game.pass_count >= 3:
                        # è®°å½•è¿ç»­Passè½®æ¬¡
                        consecutive_pass_rounds = game.pass_count
                        
                        # é‡ç½®å‡ºç‰Œæƒç»™æœ€åå‡ºç‰Œçš„ç©å®¶ï¼ˆå¦‚æœè¯¥ç©å®¶æœªå‡ºå®Œç‰Œï¼‰
                        if game.last_player is not None:
                            # ç¡®ä¿æœ€åå‡ºç‰Œçš„ç©å®¶æ²¡æœ‰å‡ºå®Œç‰Œ
                            if game.last_player not in game.ranking:
                                game.current_player = game.last_player
                                game.log(f"ç©å®¶ {game.last_player + 1} è·å¾—æ–°ä¸€è½®å‡ºç‰Œæƒï¼ˆè¿ç»­{consecutive_pass_rounds}æ¬¡Passåï¼‰ ")
                            else:
                                # å¦‚æœæœ€åå‡ºç‰Œçš„ç©å®¶å·²å‡ºå®Œç‰Œï¼Œåˆ™é€‰æ‹©ä¸‹ä¸€ä¸ªæœªå‡ºå®Œç‰Œçš„ç©å®¶
                                next_player = (game.last_player + 1) % 4
                                while next_player in game.ranking:
                                    next_player = (next_player + 1) % 4
                                game.current_player = next_player
                                game.log(f"ç©å®¶ {next_player + 1} è·å¾—æ–°ä¸€è½®å‡ºç‰Œæƒï¼ˆè¿ç»­{consecutive_pass_rounds}æ¬¡Passåï¼‰ ")
                        else:
                            # å¦‚æœæ²¡æœ‰æœ€åå‡ºç‰Œçš„ç©å®¶ï¼Œåˆ™æŒ‰é¡ºåºæ‰¾ä¸‹ä¸€ä¸ªæœªå‡ºå®Œç‰Œçš„ç©å®¶
                            next_player = (game.current_player + 1) % 4
                            while next_player in game.ranking:
                                next_player = (next_player + 1) % 4
                            game.current_player = next_player
                            game.log(f"ç©å®¶ {next_player + 1} è·å¾—æ–°ä¸€è½®å‡ºç‰Œæƒï¼ˆè¿ç»­{consecutive_pass_rounds}æ¬¡Passåï¼‰")

                        # é‡ç½®çŠ¶æ€
                        game.last_play = []
                        game.pass_count = 0  # é‡ç½®Passè®¡æ•°é¿å…æ­»å¾ªç¯
                        game.recent_actions = [['None'] for _ in range(4)]
                        game.is_free_turn = True
                        game.is_game_over = len(game.ranking) >= 4
                        continue

                    # ç©å®¶0ï¼šè®­ç»ƒä¸­çš„PPOæ™ºèƒ½ä½“ï¼ˆä¸»è®­ç»ƒå¯¹è±¡ï¼‰
                    # ç©å®¶1-3ï¼šæ¸¸æˆå†…ç½®çš„è§„åˆ™å‹AIå¯¹æ‰‹
                    # é€šè¿‡current_playerè½®è½¬æœºåˆ¶å®ç°å›åˆåˆ¶å‡ºç‰Œ
                    if game.current_player == 0:
                        # ä½¿ç”¨ç¼“å­˜æœºåˆ¶æå‡æ•ˆç‡
                        if not hasattr(game, 'cached_state') or game.cached_state is None:
                            state = game._get_obs()
                            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)
                            mask = torch.tensor(game.get_valid_action_mask(game.players[0].hand, M, game.active_level, game.last_play), dtype=torch.float32).unsqueeze(0).to(device)
                            mask = mask.squeeze(0)
                            
                            # ç¼“å­˜è®¡ç®—ç»“æœ
                            game.cached_state = state
                            game.cached_state_tensor = state_tensor
                            game.cached_mask = mask
                        else:
                            # ä½¿ç”¨ç¼“å­˜
                            state_tensor = game.cached_state_tensor
                            mask = game.cached_mask
                        
                        # ä»…åœ¨éœ€è¦æ—¶åˆ‡æ¢æ¨¡å¼
                        if actor.training:
                            actor.eval()
                        
                        with torch.no_grad():
                            probs, _ = actor(state_tensor, mask)
                            temperature = 1.1
                            adj_probs = (probs ** (1/temperature))
                            adj_probs = adj_probs / adj_probs.sum()
                            dist = Categorical(adj_probs)
                            action = dist.sample()
                            log_prob = dist.log_prob(action)
                            action_id = action.item()
                        
                        # ä¸æ¸…é™¤ç¼“å­˜ï¼Œä¿ç•™ç”¨äºåç»­æ­¥éª¤
                        entry = M_id_dict[action_id]
                        player = game.players[0]
                        hand_size_before = len(player.hand)
                        '''
                        state = game._get_obs() #å½“å‰æ¸¸æˆçŠ¶æ€
                        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)
                        mask = torch.tensor(game.get_valid_action_mask(game.players[0].hand, M,game.active_level, game.last_play), dtype=torch.float32).unsqueeze(0).to(device) #è·å–æœ‰æ•ˆåŠ¨ä½œ æ©ç å¤„ç†ï¼šæ— æ•ˆåŠ¨ä½œä½ç½®ä¸º0ï¼Œæœ‰æ•ˆåŠ¨ä½œä¸º1ï¼Œç¡®ä¿æ™ºèƒ½ä½“åªé€‰æ‹©åˆæ³•åŠ¨ä½œ
                        mask = mask.squeeze(0) 
                        actor.eval() #åˆ‡æ¢actorç½‘ç»œåˆ°è¯„ä¼°æ¨¡å¼ï¼ˆç¦ç”¨dropoutç­‰è®­ç»ƒä¸“ç”¨å±‚ï¼‰
                        with torch.no_grad():
                            probs, _ = actor(state_tensor, mask) #è¾“å‡ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒprobsï¼ˆå¿½ç•¥ä»·å€¼å‡½æ•°è¾“å‡ºï¼‰
                            temperature = 1.1 #é€šè¿‡æ¸©åº¦å‚æ•°temperature=1.1è°ƒæ•´æ¢ç´¢ç¨‹åº¦
                            adj_probs = (probs ** (1/temperature))
                            adj_probs = adj_probs / adj_probs.sum()
                            dist = Categorical(adj_probs)
                            action = dist.sample()
                            log_prob = dist.log_prob(action) #è®°å½•åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡ï¼ˆç”¨äºPPOæŸå¤±è®¡ç®—ï¼‰
                            action_id = action.item()
                        actor.train() #æ¢å¤actorç½‘ç»œåˆ°è®­ç»ƒæ¨¡å¼
                        entry = M_id_dict[action_id]
                        player = game.players[0]
                        hand_size_before = len(player.hand) #è®°å½•æ‰§è¡ŒåŠ¨ä½œå‰çš„æ‰‹ç‰Œæ•°é‡ï¼ˆç”¨äºå¥–åŠ±è®¡ç®—ï¼‰
                        '''
                        
                        # ç©å®¶çš„åŠ¨ä½œé€šè¿‡ enumerate_colorful_actions å‡½æ•°ç”Ÿæˆå¯èƒ½çš„ç»„åˆã€‚
                        # å¦‚æœæœ‰å¯é€‰åŠ¨ä½œï¼Œä»£ç éšæœºé€‰æ‹©ä¸€ä¸ªåŠ¨ä½œå¹¶æ›´æ–°æ¸¸æˆçŠ¶æ€ï¼ŒåŒ…æ‹¬ç©å®¶æ‰‹ç‰Œã€æœ€è¿‘åŠ¨ä½œè®°å½•ä»¥åŠæ¸¸æˆæ—¥å¿—ã€‚
                        # å¦‚æœç©å®¶å‡ºå®Œæ‰€æœ‰ç‰Œï¼Œä»£ç æ›´æ–°æ’åå¹¶æ£€æŸ¥æ¸¸æˆæ˜¯å¦ç»“æŸã€‚
                        combos = enumerate_colorful_actions(entry, player.hand, game.active_level) 
                        if combos:
                            chosen_move = random.choice(combos)
                            if not chosen_move:
                                game.log(f"ç©å®¶ 1 PPO Pass ")
                                game.pass_count += 1
                                game.recent_actions[0] = ['Pass']
                            else:
                                is_free_turn = False
                                game.last_play = chosen_move
                                game.last_player = 0
                                for card in chosen_move:
                                    player.played_cards.append(card)
                                    player.hand.remove(card)
                                game.log(f"ç©å®¶ 1 PPO å‡ºç‰Œ: {' '.join(chosen_move)} , å½“å‰çº§ç‰Œ {game.active_level}")
                                game.recent_actions[0] = list(chosen_move)
                                game.jiefeng = False
                                if not player.hand:
                                    game.log(f"\nğŸ‰ ç©å®¶ 1 PPO å‡ºå®Œæ‰€æœ‰ç‰Œï¼\n")
                                    game.ranking.append(0)  # ç©å®¶0çš„ç´¢å¼•ç›´æ¥ä½¿ç”¨0
                                    game.is_game_over = len(game.ranking) >= 4
                                    game.pass_count = 0
                                    
                                    # åŒé‡ç¡®è®¤ï¼šç¡®ä¿æ‰‹ç‰Œç¡®å®ä¸ºç©º
                                    if player.hand:
                                        game.log(f"âš ï¸ è­¦å‘Šï¼šç©å®¶1PPOæ‰‹ç‰Œéç©ºä½†è¢«åˆ¤å®šä¸ºç©ºï¼æ‰‹ç‰Œ: {' '.join(player.hand)} ")
                                    else:
                                        # è½®è½¬ç©å®¶å¹¶è·³å‡ºå¾ªç¯
                                        game.current_player = (game.current_player + 1) % 4
                                        continue  # è·³å‡ºå½“å‰å›åˆå¾ªç¯
                                    
                                game.pass_count = 0
                                # ç§»é™¤é‡å¤çš„handæ£€æŸ¥
                                if game.is_free_turn:
                                    game.is_free_turn = False
                        else:
                            game.log(f"ç©å®¶ 1  PPO Pass ")
                            game.pass_count += 1
                            game.recent_actions[0] = ['Pass']
                            
                        next_state = game._get_obs()
                        reward = calculate_improved_reward(entry, player, mask, action_id, hand_size_before, game, ep)
                        reward += calculate_team_reward(game)
                        episode_reward += reward
                        memory.push(state, action_id, reward, next_state, game.is_game_over, log_prob.item())
                            
                        player.last_played_cards = game.recent_actions[0]
                        game.current_player = (game.current_player + 1) % 4 
                        episode_steps += 1
                    else:
                        # æ·»åŠ AIç©å®¶ç»“æŸæ£€æŸ¥
                        current_ai_player = game.players[game.current_player]
                        game.ai_play(current_ai_player)
                        
                        # æ£€æŸ¥AIç©å®¶æ˜¯å¦å‡ºå®Œç‰Œï¼ˆä½¿ç”¨ç©å®¶ç´¢å¼•ä»£æ›¿seatå±æ€§ï¼‰
                        if not current_ai_player.hand and game.current_player not in game.ranking:
                            game.ranking.append(game.current_player)
                            game.is_game_over = len(game.ranking) >= 4
                            if game.is_game_over:
                                break  # ç«‹å³ç»“æŸæ¸¸æˆå¾ªç¯
                            
                    round_history = []
                    if game.current_player == 0 and any(action != ['None'] for action in game.recent_actions):
                        round_history = [action.copy() for action in game.recent_actions]
                        # é™åˆ¶å†å²è®°å½•é•¿åº¦
                        if len(game.history) < 50:
                            game.history.append(round_history) 
                        game.recent_actions = [['None'] for _ in range(4)]
                    
                    all_others_pass = False    
                    if len(round_history) == 4 and round_history[0] != ['Pass']:
                        all_others_pass = all(action == ['Pass'] for action in round_history[1:4])
                
                    if all_others_pass:
                        game.is_free_turn = True
                        game.last_play = []
                        game.pass_count = 0
                        if hasattr(game, 'last_player'):
                            game.current_player = game.last_player
                        #print("è‡ªç”±å‡ºç‰Œè½®ï¼Œmask:", mask.cpu().numpy())
                    
                    if all(action == ['Pass'] for action in game.recent_actions):
                        game.is_free_turn = True
                        game.last_play = []
                        game.pass_count = 0
                        print("æ­»å¾ªç¯æ£€æµ‹ï¼Œmask:", mask.cpu().numpy())
        
        collectors = []
        for i in range(num_collectors):
            memory = memory_list[i]
            collector = Thread(target=data_collection_thread, daemon=True,kwargs={'id': i})
            collector.start()
        
        # ä¸»è®­ç»ƒå¾ªç¯
        collected_episodes = 0
        mennum = 0
        while collected_episodes < episodes:
            mennum += 1
            memory = memory_list[collected_episodes%mennum]
            
            # æ‰¹é‡è®­ç»ƒ
            if len(memory) >= adaptive_params['current_batch_size']:
                states, actions, rewards, next_states, dones, old_log_probs = memory.sample(
                    adaptive_params['current_batch_size']
                )

                # ä»é˜Ÿåˆ—è·å–æ•°æ®
                collected_episodes += 1

                print(f"len1(memory):{len(memory)} ID:{collected_episodes%mennum}")
                
                # åŠ¨æ€è°ƒæ•´batch size
                if collected_episodes % adaptive_params['batch_growth_interval'] == 0:
                    adaptive_params['current_batch_size'] = min(
                        adaptive_params['max_batch_size'],
                        adaptive_params['current_batch_size'] + adaptive_params['growth_step']
                    )
                
                # ä½¿ç”¨torch.jit.scriptåŠ é€Ÿè®­ç»ƒ
                with torch.jit.optimized_execution(True):
                    policy_loss, value_loss, entropy, kl_div = train_on_batch_ppo(
                        states, actions, rewards, next_states, dones, old_log_probs,
                        backbone, actor, critic, target_critic, optimizer,
                        gamma=0.99, gae_lambda=0.97, device=device, ep=collected_episodes
                    )
                
                # è®°å½•è®­ç»ƒæŒ‡æ ‡
                writer.add_scalar('Training/PolicyLoss', policy_loss, collected_episodes)
                writer.add_scalar('Training/ValueLoss', value_loss, collected_episodes)
                writer.add_scalar('Training/Entropy', entropy, collected_episodes)
                writer.add_scalar('Training/KLDivergence', kl_div, collected_episodes)
                writer.add_scalar('Training/EpisodeSteps', collected_episodes, collected_episodes)
                
                # å‡å°‘ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡
                if collected_episodes % 50 == 0:                     
                    soft_update(target_backbone, backbone)
                    soft_update(target_critic, critic)
                    
                scheduler.step(policy_loss)
                
                # å®šæœŸæ‰“å°æ—¥å¿—    
                if (collected_episodes + 1) % 10 == 0:                
                    logging.info(
                        f"Episode {collected_episodes + 1}: "
                        f"PLoss={policy_loss:.4f}, VLoss={value_loss:.4f}, "
                        f"Entropy={entropy:.4f}, KL={kl_div:.4f}, "
                        f"BatchSize={adaptive_params['current_batch_size']}"
                    )
                
                # è®­ç»ƒåæ¸…ç†å·²ä½¿ç”¨çš„æ ·æœ¬
                # è®¡ç®—å®é™…ä½¿ç”¨çš„æ ·æœ¬ç´¢å¼•
                used_indices = set()
                for i in range(len(memory.buffer)):
                    for j in range(len(states)):
                        if np.array_equal(memory.buffer[i][0], states[j]):
                            used_indices.add(i)
                
                # ç§»é™¤å·²ä½¿ç”¨çš„æ ·æœ¬
                memory.buffer = [item for idx, item in enumerate(memory.buffer) if idx not in used_indices]
                
                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                if (collected_episodes + 1) % 100 == 0:
                    save_checkpoint(backbone, actor, critic, optimizer, collected_episodes + 1) 
            else:
                mennum += 1
                time.sleep(5)

        # ç­‰å¾…æ‰€æœ‰collectorç»“æŸ
        for collector in collectors:
            collector.join()
                
    except KeyboardInterrupt:
        logging.info("è®­ç»ƒè¢«æ‰‹åŠ¨ä¸­æ–­")
        save_checkpoint(backbone, actor, critic, optimizer, 1000000, model_dir="models/interrupted")
    finally:
        writer.close()

if __name__ == "__main__":
    logging.info("å¼€å§‹è®­ç»ƒ æ¼è›‹ PPO æ™ºèƒ½ä½“")
    logging.info(f"Pythonç‰ˆæœ¬: {sys.version}")
    logging.info(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    logging.info(f"è®¾å¤‡: {device}")
    run_training()
