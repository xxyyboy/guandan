## 2025/3/21

### Agent组：

![e8aa1fcc1e882866969500d8a43f9455_720](https://github.com/user-attachments/assets/52af1fd5-a624-4064-a36f-8939e23751b2)

![a0d43911d79d063d858809f461f2bce6](https://github.com/user-attachments/assets/3189db79-15d6-4187-8e76-a0f7147de099)

### 强化学习组：

虚拟对局中放弃`逢人配`规则、`接风`规则、`贡牌`规则、`某队升级到A后，再取得一次非一四名胜利赢得游戏`规则、`某队升级到A后，连续输或取得一四名胜利3次，降级`规则（写不出来🤡）

目前支持级牌升级规则、A的特殊用法

准备开始模型搭建，以上未支持的规则放在以后实战时学习。

模型选择：`DQN`/`PPO`/`Transformer`

**状态表示**

**使用 One-Hot 编码（或多热编码）来表示手牌和其他信息**

| 特征                  | 维度             | 说明                                                                 |
|-----------------------|------------------|----------------------------------------------------------------------|
| 当前玩家的手牌         | 108              | 54 张牌，每张牌是否在手（双份）                                       |
| 其他玩家的手牌         | 108              | 其他玩家手牌信息（可能需要模糊处理）                                   |
| 每个玩家最近动作       | 108 × 4          | 每个玩家的最近出牌                                                     |
| 其他玩家出的牌         | 108 × 3          | 记录已经打出的牌                                                       |
| 其他玩家剩余牌数       | 27 × 3           | 记录三名玩家还剩多少张牌                                               |
| 当前级牌               | 13               | 级牌 one-hot 表示                                                      |
| 最近 20 次动作         | 108 × 4 × 5      | 记录 5 轮历史，每轮 4 个玩家的出牌情况                                 |
| 协作状态               | 3                | 标识与队友的配合程度                                                   |
| 压制状态               | 3                | 标识对敌人的打压情况                                                   |
| 辅助状态               | 3                | 标识是否有意给队友铺路                                                 |

## 2025/3/14

1.学习掼蛋规则 

2.文献调研

[多智能体强化学习综述](./45c2e243172d3ca62987922e77496221.pdf)      [多Agent深度强化学习综述](./多Agent深度强化学习综述.pdf)

[AGENTVERSE FACILITATING MULTI-AGENT COLLAB](https://github.com/user-attachments/files/19284828/AGENTVERSE.FACILITATING.MULTI-AGENT.COLLAB.pdf)


[Self-collaboration Code Generation via ChatGPT](https://github.com/user-attachments/files/19284844/Self-collaboration.Code.Generation.via.ChatGPT.pdf)

[Evaluating and enhancing llms agent based on theory of mind in guandan A multi-player cooperative game under imperfect information](https://github.com/user-attachments/files/19284848/Evaluating.and.enhancing.llms.agent.based.on.theory.of.mind.in.guandan.A.multi-player.cooperative.game.under.imperfect.information.pdf)

[Large Language Model based Multi-Agents A Survey of Progress and Challenges](https://github.com/user-attachments/files/19284852/Large.Language.Model.based.Multi-Agents.A.Survey.of.Progress.and.Challenges.pdf)

[Mastering the Game of Guandan with Deep Reinforcement Learning and Behavior Regulating](https://github.com/user-attachments/files/19284853/Mastering.the.Game.of.Guandan.with.Deep.Reinforcement.Learning.and.Behavior.Regulating.pdf)


3.打算分为2组，一组做Agent，一组做强化学习。

### Agent组：

### 强化学习组：

- [x] 学习规则
- [x] 发牌程序 `give_cards.py`
- [x] 规则函数`rule.py`
- [x] 对局模拟`Game.py`

代码可参考项目：https://github.com/LSTM-Kirigaya/egg-pancake.git

![屏幕截图 2025-03-15 152918](https://github.com/user-attachments/assets/76075b01-5bdf-453b-89cf-9af0bee598d4)


训练一个掼蛋AI是一个非常有挑战性的任务，因为掼蛋不仅需要对抗性策略，还需要合作性策略。以下是几种适合处理合作与对抗场景的神经网络模型和技术建议：

---

1. **深度强化学习（Deep Reinforcement Learning, DRL）**
深度强化学习是训练游戏AI的常用方法，特别适合掼蛋这种需要动态决策的游戏。以下是几种适合的DRL模型：
- **Deep Q-Network (DQN)**：适用于离散动作空间，可以通过Q值学习来优化策略。
- **Proximal Policy Optimization (PPO)**：适合连续或高维动作空间，稳定性较高，适合掼蛋中复杂的策略调整。
- **Multi-Agent Deep Deterministic Policy Gradient (MADDPG)**：专门为多智能体设计，可以同时处理合作与对抗，适合掼蛋中玩家之间的互动。

---

2. **生成对抗网络（Generative Adversarial Networks, GANs）**
虽然GANs主要用于生成任务，但其对抗性训练机制可以借鉴到掼蛋AI中：
- **Conditional GANs**：可以生成特定条件下的策略或牌型，帮助AI在特定情境下做出决策。
- **Adversarial Training**：通过模拟对手的行为，训练AI在对抗中优化策略。

---

3. **多智能体强化学习（Multi-Agent Reinforcement Learning, MARL）**
掼蛋涉及多个玩家之间的合作与对抗，MARL是处理这类问题的理想选择：
- **Cooperative MARL**：通过共享奖励或策略，训练AI与队友合作。
- **Competitive MARL**：通过对抗性训练，提升AI在竞争中的表现。
- **Mixed Cooperative-Competitive MARL**：结合合作与对抗，适合掼蛋中复杂的交互场景。

---

4. **图神经网络（Graph Neural Networks, GNNs）**
掼蛋的牌型和玩家关系可以用图结构表示，GNNs适合处理这种结构化数据：
- **Graph Attention Networks (GATs)**：可以捕捉玩家之间的动态关系，优化合作策略。
- **Graph Convolutional Networks (GCNs)**：用于分析牌型和玩家行为的关联性。

---

5. **模仿学习（Imitation Learning）**
通过模仿人类玩家的策略，快速提升AI的表现：
- **Behavioral Cloning**：直接模仿人类玩家的决策。
- **Inverse Reinforcement Learning (IRL)**：通过观察人类行为，推断其潜在奖励函数，从而优化AI策略。

---

6. **混合模型**
结合多种技术的混合模型可能更适合掼蛋AI：
- **DRL + GNNs**：用GNNs处理牌型和玩家关系，用DRL优化策略。
- **MARL + Imitation Learning**：通过模仿学习初始化策略，再用MARL进行优化。

---

7. **对抗样本训练**
为了提高AI的鲁棒性，可以引入对抗样本训练：
- **Adversarial Examples**：通过生成对抗样本，训练AI在复杂或欺骗性情境下的表现。

---

8. **实践建议**
- **数据集**：收集大量掼蛋对局数据，包括牌型、玩家行为和胜负结果。
- **奖励设计**：设计合理的奖励函数，平衡合作与对抗的目标。
- **评估指标**：除了胜率，还可以评估AI的合作能力、策略多样性和鲁棒性。
