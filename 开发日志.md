## 2025/6/2
添加记牌器
## 2025/5/25
新增建议数选项、修复已知问题
## 2025/5/23
用Vue重构了设置页、单人对战前端

## 2025/5/2 - 5/13
- 5/13: 重写README、调整奖励函数
- 5/12: 因域名问题暂停联机开发
- 5/11: 优化前端、修复已知问题
- 5/9: 修正奖励函数(添加拆牌惩罚), 美化前端
- 5/2: 使用`streamlit`制作前端
## 2025/4/26
修复了`enumerate_colorful_actions`识别相同牌出错的问题
## 2025/4/26
修复了掩码问题
## 2025/4/25
### 建立A2C网络，能够训练、预测。还剩掩码处可能存在小问题

`ActorNet` 输出结构动作 `action_id` 的概率分布（考虑了 mask）；

`CriticNet` 输出当前 `state` 的 `value` 估计；
### 奖励函数

r=reward + gamma ** (len(memory) - i - 1) * final_reward

`reward`为即时奖励，值为出牌长度/牌型的`logic_point`

`final_reward`为整局奖励，队伍获胜为正，12名为3，13名为2，14名为1，反之亦然

`memory`是一局的记录，结构为`[{state,action_id,reward},{state,action_id,reward},...]`


### 优势函数：

$advantage=r+\gamma*V(state')-V(state)$

其中最后一个 $transition$ 没有 $state'$ ,设 $\gamma*V(state')=0$

`Actor Loss`= $-log(p)*advantage$

`Critic Loss` = $||advantage||^2$
## 2025/4/20
108维具体参照：
`
{[黑桃2-A]+[红桃2-A]+[梅花2-A]+[方块2-A]}*2+[小王,大王]*2
`
>e.g.
> 
>hand = ['红桃2', '红桃2','黑桃3', '红桃3', '黑桃4', '红桃4', '红桃4', '黑桃4','黑桃5', '红桃5','大王','小王','大王']
> 
> obs:
> 
> [0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1.]

## 2025/4/18
### 强化学习组：
搭建Actor网络（暂无奖励函数）

## 2025/4/7
### 强化学习组：
✅ 添加`接风`规则支持

## 2025/4/7
### 强化学习组：
![graphviz.png](graphviz.png)

## 2025/3/30

### 强化学习组：

不再使用DQN（状态太多），转换为PPO+ Actor-Critic

 **PPO（Proximal Policy Optimization，近端策略优化）+ Actor-Critic（评论家-演员）算法**

PPO 是一种强化学习算法，它基于 **策略梯度（Policy Gradient）**，并通过 **裁剪（Clipping）** 使得训练更加稳定。  
而 **Actor-Critic（AC，演员-评论家）** 是强化学习的一种架构，结合了 **值函数（Critic，评论家）** 和 **策略梯度（Actor，演员）**，以提高学习效率。

---

 **1. PPO + Actor-Critic 结构**
PPO 结合了 **Actor-Critic**，其中：
- **Actor（策略网络）**：负责决策，给定状态后，输出一个策略分布 $\pi_{\theta}(a|s)$。
- **Critic（价值网络）**：负责评估，计算状态值 $\(V(s)\)$ 估计当前状态的长期收益。

训练目标：
1. **Actor 训练目标** → 让策略在不剧烈变化的情况下，提高预期回报
2. **Critic 训练目标** → 让 Critic 准确估计状态值 $\(V(s)\)$

---

 **2. PPO 核心思想**
在策略梯度方法中，我们希望优化 **策略**（Actor），但如果策略变化过大，可能会导致学习不稳定。因此，PPO 采用 **策略约束** 来限制更新步长：

 **1️⃣ 旧策略与新策略的比值**
PPO 通过计算 **新旧策略的比值** 来控制更新：
$\[
r_t(\theta) = \frac{\pi_{\theta}(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)}
\]$
- 如果 $\( r_t(\theta) > 1 \)$，说明新策略比旧策略更倾向选择这个动作
- 如果 $\( r_t(\theta) < 1 \)$，说明新策略减少了选择这个动作的概率

**2️⃣ 目标函数（裁剪损失）**
PPO 使用 **裁剪（Clipping）** 技术，防止策略变化过大：
$\[
L(\theta) = \min\left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) A_t \right)
\]$
其中：
- $\( A_t \)$ 是 **优势函数**，衡量当前动作比平均水平好多少
- $\( \epsilon \$) 是裁剪范围（通常设为 0.2）
- 这个函数的作用是：
  - 当 $\( r_t(\theta) \)$ 变化太大时，直接限制其梯度，避免训练崩溃

 **3️⃣ Critic 的目标**
Critic 通过最小化 **均方误差（MSE）** 来优化值函数：
$\[
L_{\text{critic}} = (V(s) - \text{target})^2
\]$
其中：
- **target** 一般用 **TD 目标**（Temporal Difference Target）：  
  $\[
  \text{target} = r + \gamma V(s')
  \]$
  这表示当前状态的价值等于即时奖励 + 折扣后的下一个状态价值。

---

 **3. PPO 训练流程**
1. **初始化** Actor（策略网络）和 Critic（值网络）
2. **采集数据**：让 AI **自我对局**，收集 **状态-动作-奖励**
3. **计算优势**：计算 $\( A_t = Q(s,a) - V(s) \)$ 作为训练信号
4. **更新 Actor（策略）**：使用裁剪损失优化
5. **更新 Critic（值网络）**：最小化均方误差
6. **重复训练，直到策略收敛**

---

**4. PPO 的优点**
✅ **稳定性强**：裁剪策略防止过度更新，减少策略崩溃  
✅ **样本利用率高**：PPO 可以 **重复使用样本** 进行训练，提高数据效率  
✅ **适用于高维控制问题**：能用于复杂决策任务（如掼蛋 AI）  

---

 **5. 为什么 PPO 适合训练掼蛋 AI？**
1. **对局复杂度高**：掼蛋是 **不完全信息博弈**，PPO 适用于决策场景  
2. **动作空间大**：每个回合 AI 需要选择一个合法的牌型，PPO 可以通过 **策略网络** 输出分布来选择合适的牌  
3. **长期规划**：AI 需要在 **多个回合** 内做出最优决策，而 Critic 网络可以帮助 AI 评估局势  

---

 **6. 你接下来可以做什么？**
- **定义状态表示**（参考你上传的图片，编码游戏状态）
- **搭建 Actor-Critic 网络**
- **实现 PPO 训练流程**
- **让 AI 自我对局，训练强化学习模型**


## 2025/3/21

### Agent组：

![e8aa1fcc1e882866969500d8a43f9455_720](https://github.com/user-attachments/assets/52af1fd5-a624-4064-a36f-8939e23751b2)

![a0d43911d79d063d858809f461f2bce6](https://github.com/user-attachments/assets/3189db79-15d6-4187-8e76-a0f7147de099)

### 强化学习组：

虚拟对局中放弃`逢人配`规则、`接风`规则、`贡牌`规则、`某队升级到A后，再取得一次非一四名胜利赢得游戏`规则、`某队升级到A后，连续输或取得一四名胜利3次，降级`规则（写不出来🤡）

目前支持级牌升级规则、A的特殊用法

准备开始模型搭建，以上未支持的规则放在以后实战时学习。

模型选择：`DQN`/`PPO`/`Transformer`

**状态表示**

**使用 One-Hot 编码（或多热编码）来表示手牌和其他信息**

| 特征                  | 维度             | 说明                                                                 |
|-----------------------|------------------|----------------------------------------------------------------------|
| 当前玩家的手牌         | 108              | 54 张牌，每张牌是否在手（双份）                                       |
| 其他玩家的手牌         | 3              | 3 维，表示剩余手牌数量（归一化））                                   |
| 每个玩家最近动作       | 108 × 4          | 每个玩家的最近出牌                                                     |
| 其他玩家出的牌         | 108 × 3          | 记录已经打出的牌                                                       |
| 当前级牌               | 13               | 级牌 one-hot 表示                                                      |
| 最近 20 次动作         | 108 × 4 × 5      | 5 轮历史，每轮 4 玩家，每人 108 维                                 |
| 协作状态               | 3                | 标识与队友的配合程度                                                   |
| 压制状态               | 3                | 标识对敌人的打压情况                                                   |
| 辅助状态               | 3                | 标识是否有意给队友铺路                                                 |
|总维度                   | `3049`         |   |

## 2025/3/14

1.学习掼蛋规则 

2.文献调研

[多智能体强化学习综述](./45c2e243172d3ca62987922e77496221.pdf)      [多Agent深度强化学习综述](./多Agent深度强化学习综述.pdf)

[AGENTVERSE FACILITATING MULTI-AGENT COLLAB](https://github.com/user-attachments/files/19284828/AGENTVERSE.FACILITATING.MULTI-AGENT.COLLAB.pdf)


[Self-collaboration Code Generation via ChatGPT](https://github.com/user-attachments/files/19284844/Self-collaboration.Code.Generation.via.ChatGPT.pdf)

[Evaluating and enhancing llms agent based on theory of mind in guandan A multi-player cooperative game under imperfect information](https://github.com/user-attachments/files/19284848/Evaluating.and.enhancing.llms.agent.based.on.theory.of.mind.in.guandan.A.multi-player.cooperative.game.under.imperfect.information.pdf)

[Large Language Model based Multi-Agents A Survey of Progress and Challenges](https://github.com/user-attachments/files/19284852/Large.Language.Model.based.Multi-Agents.A.Survey.of.Progress.and.Challenges.pdf)

[Mastering the Game of Guandan with Deep Reinforcement Learning and Behavior Regulating](https://github.com/user-attachments/files/19284853/Mastering.the.Game.of.Guandan.with.Deep.Reinforcement.Learning.and.Behavior.Regulating.pdf)


3.打算分为2组，一组做Agent，一组做强化学习。

### Agent组：

### 强化学习组：

- [x] 学习规则
- [x] 发牌程序 `give_cards.py`
- [x] 规则函数`rule.py`
- [x] 对局模拟`Game.py`

代码可参考项目：https://github.com/LSTM-Kirigaya/egg-pancake.git

![屏幕截图 2025-03-15 152918](https://github.com/user-attachments/assets/76075b01-5bdf-453b-89cf-9af0bee598d4)


训练一个掼蛋AI是一个非常有挑战性的任务，因为掼蛋不仅需要对抗性策略，还需要合作性策略。以下是几种适合处理合作与对抗场景的神经网络模型和技术建议：

---

1. **深度强化学习（Deep Reinforcement Learning, DRL）**
深度强化学习是训练游戏AI的常用方法，特别适合掼蛋这种需要动态决策的游戏。以下是几种适合的DRL模型：
- **Deep Q-Network (DQN)**：适用于离散动作空间，可以通过Q值学习来优化策略。
- **Proximal Policy Optimization (PPO)**：适合连续或高维动作空间，稳定性较高，适合掼蛋中复杂的策略调整。
- **Multi-Agent Deep Deterministic Policy Gradient (MADDPG)**：专门为多智能体设计，可以同时处理合作与对抗，适合掼蛋中玩家之间的互动。

---

2. **生成对抗网络（Generative Adversarial Networks, GANs）**
虽然GANs主要用于生成任务，但其对抗性训练机制可以借鉴到掼蛋AI中：
- **Conditional GANs**：可以生成特定条件下的策略或牌型，帮助AI在特定情境下做出决策。
- **Adversarial Training**：通过模拟对手的行为，训练AI在对抗中优化策略。

---

3. **多智能体强化学习（Multi-Agent Reinforcement Learning, MARL）**
掼蛋涉及多个玩家之间的合作与对抗，MARL是处理这类问题的理想选择：
- **Cooperative MARL**：通过共享奖励或策略，训练AI与队友合作。
- **Competitive MARL**：通过对抗性训练，提升AI在竞争中的表现。
- **Mixed Cooperative-Competitive MARL**：结合合作与对抗，适合掼蛋中复杂的交互场景。

---

4. **图神经网络（Graph Neural Networks, GNNs）**
掼蛋的牌型和玩家关系可以用图结构表示，GNNs适合处理这种结构化数据：
- **Graph Attention Networks (GATs)**：可以捕捉玩家之间的动态关系，优化合作策略。
- **Graph Convolutional Networks (GCNs)**：用于分析牌型和玩家行为的关联性。

---

5. **模仿学习（Imitation Learning）**
通过模仿人类玩家的策略，快速提升AI的表现：
- **Behavioral Cloning**：直接模仿人类玩家的决策。
- **Inverse Reinforcement Learning (IRL)**：通过观察人类行为，推断其潜在奖励函数，从而优化AI策略。

---

6. **混合模型**
结合多种技术的混合模型可能更适合掼蛋AI：
- **DRL + GNNs**：用GNNs处理牌型和玩家关系，用DRL优化策略。
- **MARL + Imitation Learning**：通过模仿学习初始化策略，再用MARL进行优化。

---

7. **对抗样本训练**
为了提高AI的鲁棒性，可以引入对抗样本训练：
- **Adversarial Examples**：通过生成对抗样本，训练AI在复杂或欺骗性情境下的表现。

---

8. **实践建议**
- **数据集**：收集大量掼蛋对局数据，包括牌型、玩家行为和胜负结果。
- **奖励设计**：设计合理的奖励函数，平衡合作与对抗的目标。
- **评估指标**：除了胜率，还可以评估AI的合作能力、策略多样性和鲁棒性。
